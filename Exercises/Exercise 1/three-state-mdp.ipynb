{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup the notebook: During the course of exercises, we will require several python packages to be installed. You can find all necessary requirements in the attached requirements.txt file. As a suggestion, you can create a virtual environment and install the requirements there (see https://docs.python.org/3/library/venv.html). A virtual environment can be constructed and activated using\n",
    "\n",
    "```console\n",
    "python3 -m venv /path/to/new/virtual/environment\n",
    "source /path/to/new/virtual/environment/bin/activate\n",
    "```\n",
    "\n",
    "You can then install the requirements using\n",
    "\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three state MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the three state MDP shown on the exercise set as a `gym` environment.\n",
    "\n",
    "For more information on `gym`, visit https://gymnasium.farama.org\n",
    "\n",
    "A `gym` environment can be thought of as a **simulator** of the underlying MDP.\n",
    "This means that we don't need to know the actual dynamics distribution, but we\n",
    "only need to implement what happens when we take some action in some state. Most\n",
    "practical applications of Reinforcement Learning only have access to such\n",
    "simulators, as we will see later in the lecture.\n",
    "\n",
    "A `gym` environment must provide the following:\n",
    "- An `observation_space` that describes the set of states.  \n",
    "  *Note: `gym` supports partially observable Markov decision processes (POMDPs),\n",
    "  which are more general than MDPs. In the partially observable setting, we\n",
    "  cannot observe the full state, but only a limited observation of the state. In\n",
    "  our case the states are fully observable, i.e., the observation space is the\n",
    "  same as the state space.*\n",
    "- An `action_space` that describes the set of actions.\n",
    "- A `reset()` method that resets the state of the environment and returns this state.\n",
    "- A `step(action)` method that performs a transition from the current state and\n",
    "  the given action to the next state. It returns the next state and the reward\n",
    "  that was produced during this transition.\n",
    "\n",
    "We already started the implementation below, just fill in the missing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeStateMDP(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Define the observation space for the three states.\n",
    "        # The states are encoded as integers, i.e., X = 0, Y = 1, Z = 2.\n",
    "        self.observation_space = gym.spaces.Discrete(3)\n",
    "\n",
    "        # Define the action space for the two actions.\n",
    "        # The actions are encoded as integers, i.e., left = 0, right = 1.\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "\n",
    "        # Create an attribute for the current state of this environment\n",
    "        # (initially empty).\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        #######################################################################\n",
    "        # TODO: This method resets the internal state of this environment     #\n",
    "        # according to the initial state distribution. Remember that the      #\n",
    "        # states are encoded as integers (see __init__).                      #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "\n",
    "        # The gym API requires two return values:\n",
    "        # - The current state of the environment\n",
    "        # - An `info` dictionary containing additional reset information, which\n",
    "        #   is empty in our case\n",
    "        info = {}\n",
    "        return self.state, info\n",
    "\n",
    "    def step(self, action):\n",
    "        #######################################################################\n",
    "        # TODO: This methods transitions from the current state of the        #\n",
    "        # environment and the provided action to a next state and produces a  #\n",
    "        # reward. gym also explicitly differentiates between non-terminal and #\n",
    "        # terminal states since this simplifies a lot of our code. For this   #\n",
    "        # reason, we also have to return a `terminated` flag that indicates   #\n",
    "        # whether the new state is terminal (i.e., the episode ended in a     #\n",
    "        # terminal state). You can generate a random number between 0 and 1   #\n",
    "        # using random.random()                                               #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "\n",
    "        # In addition to the next state, reward, and the terminated flag, the\n",
    "        # gym API requires two more return values:\n",
    "        # - A `truncated` flag that indicates whether the episode was truncated,\n",
    "        #   i.e., whether it ended early for some other reason than reaching a\n",
    "        #   terminal state (e.g. when a maximum number of steps is reached),\n",
    "        #   which is always False in our case\n",
    "        # - An `info` dictionary containing additional step information, which\n",
    "        #   is empty in our case\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        return self.state, reward, terminated, truncated, info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation we can do the following:\n",
    "1. Sample an episode with some policy (which we call performing a \"rollout\") and\n",
    "   compute the sum of all rewards.\n",
    "2. Repeat this for $n$ episodes and compute the average of these sums.\n",
    "\n",
    "We already implemented the function `average_reward_sum(env, policy, n)` that\n",
    "takes a `gym` environment and a policy, and returns the average sum of rewards\n",
    "by sampling `n` episodes. The `policy` must be a function that returns an action\n",
    "given a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_reward_sum(env, policy, n):\n",
    "    total_reward_sum = 0\n",
    "    for i in range(n):\n",
    "        # Start a new episode, reset the environment\n",
    "        state, info = env.reset()\n",
    "        reward_sum = 0\n",
    "        while True:\n",
    "            # Select an action using the provided policy\n",
    "            action = policy(state)\n",
    "            # Perform an environment step\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            # Add the reward to the reward sum\n",
    "            reward_sum += reward\n",
    "            if terminated or truncated:\n",
    "                # Exit the loop when the episode ended in a terminal state or was\n",
    "                # truncated early\n",
    "                break\n",
    "        total_reward_sum += reward_sum\n",
    "    # Compute the average over n episodes\n",
    "    result = total_reward_sum / n\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented a random policy that selects left or right with a coin flip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    if random.random() < 0.5:\n",
    "        return 0  # left with 50% probability\n",
    "    else:\n",
    "        return 1  # right with 50% probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented the environment correctly, you should get an average sum of\n",
    "rewards of approximately 10 (for the random policy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ThreeStateMDP()\n",
    "average_reward_sum(env, random_policy, n=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the policies $\\pi_1$ and $\\pi_3$ from the exercise set.\n",
    "Remember the definition of the deterministic policy $\\pi_1$\n",
    "$$\\begin{aligned}\n",
    "  \\pi_1(X) & = \\text{right} \\\\\n",
    "  \\pi_1(Y) & = \\text{right}\n",
    "\\end{aligned}$$\n",
    "and the stochastic policy $\\pi_3$\n",
    "$$\\begin{aligned}\n",
    "  \\pi_3(\\text{left}|X) & = 0 & \\pi_3(\\text{left}|Y) & = 0.9 \\\\\n",
    "  \\pi_3(\\text{right}|X) & = 1 & \\pi_3(\\text{right}|Y) & = 0.1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi1(state):\n",
    "    #######################################################################\n",
    "    # TODO: Select an action based on the definition of pi1.              #\n",
    "    #######################################################################\n",
    "    \n",
    "    #######################################################################\n",
    "    # End of your code.                                                   #\n",
    "    #######################################################################\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate policy $\\pi_1$ by computing the average reward sum. What value do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ThreeStateMDP()\n",
    "average_reward_sum(env, pi1, n=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution (spoiler)</summary>\n",
    "The value should be around 6\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi3(state):\n",
    "    #######################################################################\n",
    "    # TODO: Select an action based on the definition of pi3.              #\n",
    "    #######################################################################\n",
    "    \n",
    "    #######################################################################\n",
    "    # End of your code.                                                   #\n",
    "    #######################################################################\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate policy $\\pi_3$ by computing the average reward sum. What value do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ThreeStateMDP()\n",
    "average_reward_sum(env, pi3, n=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution (spoiler)</summary>\n",
    "The value should be around 42\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
