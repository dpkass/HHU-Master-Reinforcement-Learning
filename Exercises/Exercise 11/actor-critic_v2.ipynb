{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor-Critic\n",
    "\n",
    "**Policy Gradient with Deep Learning**:  \n",
    "In general, the update rule of policy gradient can be written as:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\, \\Phi_t \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n",
    "$$\n",
    "where $\\theta$ is the weight vector, $\\alpha$ is the learning rate, and $\\Phi_t \\in \\mathbb{R}$ indicates how good it was to select the action $a_t$ in the state $s_t$.  \n",
    "But how can this applied to deep learning, where $\\pi_\\theta(a_t | s_t)$ is computed by a neural network with weights $\\theta$?  \n",
    "The solution is to write down a \"pseudo\" loss function that will result in the same gradients via backpropagation:\n",
    "$$\n",
    "\\mathcal{L}_\\text{PG}(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ -\\Phi_t \\log \\pi_\\theta(A_t | S_t) \\right]\n",
    "$$\n",
    "This means that we don't have to compute the score function anymore, but only the log-probabilities,  \n",
    "and the automatic differentiation library (e.g., PyTorch) will do the rest for us.\n",
    "\n",
    "**Advantage Actor-Critic**:  \n",
    "In the lecture we have seen that the high variance of policy gradients can be reduced with a critic (= learned value function).  \n",
    "Advantage actor-critic approximates the advantage function with the TD error and uses it for $\\Phi_t$, i.e.,\n",
    "$$\n",
    "\\Phi_t = \\underbrace{r_{t+1} + \\gamma \\hat{v}(s_{t+1}, w)}_\\text{TD target} - \\hat{v}(s_t, w)\n",
    "$$\n",
    "where $\\hat{v}$ is a learned value function with weights $w$. The value function can be trained with any VFA method.  \n",
    "We will consider the usual mean squared error between the TD target and the prediction (semi-gradient TD):\n",
    "$$\n",
    "\\mathcal{L}_\\text{VF}(w) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\text{sg}(\\underbrace{R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w)}_\\text{TD target}) - \\hat{v}(S_t, w) \\right]\n",
    "$$\n",
    "where $\\text{sg}$ is the stop-gradient operator that indicates that no gradients will be backpropagated at this point (= `detach()` in PyTorch).  \n",
    "Note that the TD target is used in both losses and only needs to be computed once.\n",
    "\n",
    "In summary, advantage actor-critic trains two neural networks:\n",
    "- *Actor*: The policy network that computes $\\pi_\\theta(a | s)$.\n",
    "- *Critic*: The value function network that computes $\\hat{v}(s, w)$.\n",
    "\n",
    "**Softmax Policy**:  \n",
    "We will again use a softmax policy, where the scores (also called \"logits\") are computed by the neural network,  \n",
    "which are then normalized to a probability distribution using the softmax function.\n",
    "\n",
    "More formally, the actor network $f_\\theta$ computes a vector of scores $y = f_\\theta(s) \\in \\mathbb{R}^M$, where $M$ is the number of actions.  \n",
    "Then the policy is computed with:\n",
    "$$\n",
    "\\pi_\\theta(a | s) = \\frac{\\exp(y_a)}{\\sum_{a'} \\exp(y_{a'})}\n",
    "$$\n",
    "This can be implemented efficiently using the `torch.softmax()` function.  \n",
    "(https://pytorch.org/docs/stable/generated/torch.softmax.html)\n",
    "\n",
    "For the loss function of the actor, we need to compute the logarithm of the policy:  \n",
    "$$\n",
    "\\log \\pi_\\theta(a | s) = y_a - \\log \\sum_{a'} \\exp(y_{a'})\n",
    "$$\n",
    "The first term selects the score of the selected action.  \n",
    "The second term can be implemented efficiently and numerically stable using the `torch.logsumexp()` function.  \n",
    "(https://pytorch.org/docs/stable/generated/torch.logsumexp.html)\n",
    "\n",
    "**Stochastic Gradient Descent**:  \n",
    "Deep neural networks are trained with stochastic gradient descent, where *batches* of states, actions, and rewards are used for each update.  \n",
    "For DQN we can use experience replay to create batches from old transitions that we have collected, since Q-learning is an off-policy method.  \n",
    "But in the case of on-policy policy gradient, we cannot reuse old transitions and always have to use new data from the environment.\n",
    "\n",
    "A common approach to obtain batches for the on-policy setting is to create *multiple instances* of the environment.  \n",
    "For example, we can create $16$ instances of the same environment and call the `step()` function for all of them at each iteration.  \n",
    "\n",
    "**Algorithm**:  \n",
    "To summarize, this is a rough description of the algorithm we will use:  \n",
    "- Create $N$ environment instances\n",
    "- Reset all environment instances to obtain a batch of $N$ states\n",
    "- Loop forever:  \n",
    "  - Select a batch of $N$ actions using the policy $\\pi_\\theta$\n",
    "  - Sample transitions from all environment instances, to obtain a batch of rewards and next states\n",
    "  - Update the actor and critic network by minimizing the losses $\\mathcal{L}_\\text{PG}$ and $\\mathcal{L}_\\text{VF}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Make sure that the files `rl_gui.py` and `rl_tests.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:55:03.340377Z",
     "start_time": "2025-01-27T14:55:01.266740Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import rl_gui\n",
    "import rl_tests"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor-Critic Agent\n",
    "\n",
    "The class `AdvantageActorCritic` is a subclass of `torch.nn.Module`, which takes care of the automatic differentiation.  \n",
    "It contains both networks for the actor and the critic.  \n",
    "Your task is to finish the implementation.\n",
    "\n",
    "We provide two utility functions that might be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:55:03.344676Z",
     "start_time": "2025-01-27T14:55:03.341489Z"
    }
   },
   "source": [
    "def select_at_index(x, indices):\n",
    "    # Extracts the elements from x at the specified indices, i.e.,\n",
    "    # result[i] = x[i, indices[i]] for all i.\n",
    "    # - x has shape (batch_size, d)\n",
    "    # - indices has shape (batch_size)\n",
    "    # - the result has shape (batch_size)\n",
    "    return x.gather(-1, indices.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "def sample_categorical(probs):\n",
    "    # Samples from a categorical distribution (e.g. with probabilities computed with softmax).\n",
    "    # - probs has shape (batch_size, d)\n",
    "    # - the result has shape (batch_size)\n",
    "    return torch.multinomial(probs, 1, replacement=True).squeeze(1)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:55:03.353535Z",
     "start_time": "2025-01-27T14:55:03.345573Z"
    }
   },
   "source": [
    "class AdvantageActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, num_actions, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma  # discount factor\n",
    "\n",
    "        # actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "        # critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def select_actions(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # compute the scores for the given states\n",
    "        scores = self.actor(states)\n",
    "        # scores has shape (batch_size, num_actions)\n",
    "        #######################################################################\n",
    "        # TODO Convert the computed scores to probabilities with the softmax  #\n",
    "        # function and sample a batch of actions.                             #\n",
    "        #######################################################################\n",
    "\n",
    "        probs = torch.softmax(scores, dim=1)\n",
    "        actions = sample_categorical(probs)\n",
    "\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        return actions\n",
    "\n",
    "    def compute_log_probs(self, states, actions):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # actions has shape (batch_size)\n",
    "        # compute the scores for the given states\n",
    "        scores = self.actor(states)\n",
    "        # scores has shape (batch_size, num_actions)\n",
    "        #######################################################################\n",
    "        # TODO Compute the log-probabilities as described above for the given #\n",
    "        # batches of states and actions. You can use one of the utility       #\n",
    "        # functions and torch.logsumexp().                                    #\n",
    "        #######################################################################\n",
    "\n",
    "        log_probs_all = scores - torch.logsumexp(scores, dim=1, keepdim=True)\n",
    "        log_probs = select_at_index(log_probs_all, actions)\n",
    "\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        return log_probs\n",
    "\n",
    "    def compute_values(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # compute values for the batch of states using the critic network\n",
    "        values = self.critic(states).squeeze(-1)\n",
    "        # values has shape (batch_size)\n",
    "        return values\n",
    "\n",
    "    def compute_td_targets(self, rewards, next_states, terminations):\n",
    "        # next_states has shape (batch_size, state_dim)\n",
    "        # rewards and terminations have shape (batch_size)\n",
    "\n",
    "        gamma = self.gamma  # discount factor\n",
    "\n",
    "        # turn off gradient computation\n",
    "        with torch.no_grad():\n",
    "            #######################################################################\n",
    "            # TODO Compute the batch of TD targets, i.e., the reward plus the     #\n",
    "            # discounted value of the next state. Remember that the next value    #\n",
    "            # should not be used for terminal states.                             #\n",
    "            #######################################################################\n",
    "\n",
    "            td_target = rewards + gamma * ~terminations * self.compute_values(next_states)\n",
    "\n",
    "            #######################################################################\n",
    "            # End of your code.                                                   #\n",
    "            #######################################################################\n",
    "\n",
    "        return td_target\n",
    "\n",
    "    def compute_loss(self, states, actions, td_targets):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # actions, rewards, next_states, and terminations have shape (batch_size)\n",
    "        #######################################################################\n",
    "        # TODO Compute the loss for the actor (L_PG) and the critic (L_VF) as #\n",
    "        # described above. The expectations are approximated by taking the    #\n",
    "        # average over the batch (i.e., use `x.mean()`).                      #\n",
    "        # IMPORTANT: Apply the stop-gradient operator to the advantages (phi) #\n",
    "        # by using `x.detach()`, since we don't want to backpropagate from    #\n",
    "        # the loss of the actor to the critic network.                        #\n",
    "        #######################################################################\n",
    "\n",
    "        values = self.compute_values(states)\n",
    "        advantages = (td_targets - values).detach()\n",
    "        actor_loss = (-advantages * self.compute_log_probs(states, actions)).mean()\n",
    "        critic_loss = advantages.mean()\n",
    "\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        # return both losses\n",
    "        return actor_loss, critic_loss"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:55:05.639021Z",
     "start_time": "2025-01-27T14:55:04.017113Z"
    }
   },
   "source": [
    "def test_actor_critic():\n",
    "    torch.manual_seed(42)\n",
    "    rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "    state_dim = 5\n",
    "    num_actions = 3\n",
    "    gamma = 0.8\n",
    "    ac = AdvantageActorCritic(state_dim, num_actions, gamma)\n",
    "    with torch.no_grad():\n",
    "        for layer in (ac.actor[0], ac.actor[2], ac.actor[4], ac.critic[0], ac.critic[2], ac.critic[4]):\n",
    "            mean = rng.uniform(-0.5, 0.5)\n",
    "            layer.weight[:] = torch.as_tensor(rng.normal(mean, 0.1, layer.weight.shape))\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    sample_states = lambda batch_size: torch.as_tensor(rng.standard_normal((batch_size, state_dim), dtype=np.float32))\n",
    "    sample_actions = lambda batch_size: torch.as_tensor(rng.choice(num_actions, batch_size))\n",
    "\n",
    "    yield 'select_actions()'\n",
    "    for expected_actions in [\n",
    "        [0, 1, 0, 2, 1, 0, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0]\n",
    "    ]:\n",
    "        batch_size = len(expected_actions)\n",
    "        states = sample_states(batch_size)\n",
    "        actions = ac.select_actions(states)\n",
    "        #print(actions)\n",
    "        #print(torch.as_tensor(expected_actions))\n",
    "        if (yield from rl_tests.check_torch_tensor(actions, 'actions', shape=(batch_size,), dtype=torch.long)):\n",
    "            yield torch.all(actions == torch.as_tensor(expected_actions)).item(), 'actions are incorrect (can only be tested if you use torch.multinomial)'\n",
    "        yield None\n",
    "\n",
    "    yield 'compute_log_probs()'\n",
    "    for expected_log_probs in [\n",
    "        [-1.37329102e-04, -1.10322797e+00, -6.56127930e-04],\n",
    "        [-1.14602625, -14.17874146, -44.21386719,   0.00000000,  -1.09861231],\n",
    "        [-1.52587891e-05, -1.09861231e+00]\n",
    "    ]:\n",
    "        batch_size = len(expected_log_probs)\n",
    "        expected_log_probs = torch.as_tensor(expected_log_probs)\n",
    "        states = sample_states(batch_size)\n",
    "        actions = sample_actions(batch_size)\n",
    "        log_probs = ac.compute_log_probs(states, actions)\n",
    "        if (yield from rl_tests.check_torch_tensor(log_probs, 'log_probs', shape=(batch_size,), dtype=torch.float32)):\n",
    "            yield torch.allclose(log_probs, torch.as_tensor(expected_log_probs)), f'log_probs are incorrect (error = {torch.sum(torch.abs(log_probs - expected_log_probs)).item()})'\n",
    "        yield None\n",
    "\n",
    "    yield 'compute_td_targets()'\n",
    "    for expected_td_targets in [\n",
    "        [-1.1166, -0.3763,  0.1678, -0.1645],\n",
    "        [-1.0284, -0.8361,  1.0379, -1.6344,  1.1361],\n",
    "        [-0.9384,  0.2974, -1.0340, -0.8565, -0.1211, -0.9041, -0.6611]\n",
    "    ]:\n",
    "        batch_size = len(expected_td_targets)\n",
    "        expected_td_targets = torch.as_tensor(expected_td_targets)\n",
    "        rewards = torch.randn(batch_size, dtype=torch.float32)\n",
    "        next_states = sample_states(batch_size)\n",
    "        terminations = torch.rand(batch_size) < 0.5\n",
    "        td_targets = ac.compute_td_targets(rewards, next_states, terminations)\n",
    "        if (yield from rl_tests.check_torch_tensor(td_targets, 'td_targets', shape=(batch_size,), dtype=torch.float32)):\n",
    "            yield torch.allclose(td_targets, torch.as_tensor(expected_td_targets), atol=1e-04), f'td_targets are incorrect (error = {torch.sum(torch.abs(td_targets - expected_td_targets)).item()})'\n",
    "        yield None\n",
    "\n",
    "    yield 'compute_loss()'\n",
    "    for batch_size, expected_actor_loss, expected_critic_loss in [\n",
    "        (4, 8.9347, 0.5728),\n",
    "        (8, 0.2931, 0.5197),\n",
    "        (12, -2.5311, 0.9795)\n",
    "    ]:\n",
    "        states = sample_states(batch_size)\n",
    "        actions = sample_actions(batch_size)\n",
    "        td_targets = torch.randn(batch_size, dtype=torch.float32)\n",
    "        actor_loss, critic_loss = ac.compute_loss(states, actions, td_targets)\n",
    "        if (yield from rl_tests.check_torch_tensor(actor_loss, 'actor_loss', shape=tuple(), dtype=torch.float32)):\n",
    "            expected_actor_loss = torch.tensor(expected_actor_loss)\n",
    "            yield torch.isclose(actor_loss, expected_actor_loss, atol=1e-04), f'actor_loss is incorrect (error = {(actor_loss - expected_actor_loss).item()})'\n",
    "            yield actor_loss.requires_grad, 'gradients disabled for actor_loss'\n",
    "        if (yield from rl_tests.check_torch_tensor(critic_loss, 'critic_loss', shape=tuple(), dtype=torch.float32)):\n",
    "            expected_critic_loss = torch.tensor(expected_critic_loss)\n",
    "            yield torch.isclose(critic_loss, expected_critic_loss, atol=1e-04), f'critic_loss is incorrect (error = {(actor_loss - expected_critic_loss).item()})'\n",
    "            yield critic_loss.requires_grad, 'gradients disabled for critic_loss'\n",
    "        yield None\n",
    "\n",
    "    yield 'gradients'\n",
    "    optimizer = optim.SGD(ac.parameters(), lr=0.1)\n",
    "    states = sample_states(batch_size)\n",
    "    actions = sample_actions(batch_size)\n",
    "    td_targets = torch.randn(batch_size, dtype=torch.float32)\n",
    "\n",
    "    critic_params = tuple(param.data.clone() for param in ac.critic.parameters())\n",
    "    optimizer.zero_grad()\n",
    "    actor_loss, critic_loss = ac.compute_loss(states, actions, td_targets)\n",
    "    actor_loss.backward()\n",
    "    optimizer.step()\n",
    "    yield all(torch.all(new == old).item() for new, old in zip(ac.critic.parameters(), critic_params)), \\\n",
    "        'The weights of the critic network have changed after minimizing the actor_loss. Did you use x.detach() on the advantages?'\n",
    "\n",
    "    actor_params = tuple(param.data.clone() for param in ac.actor.parameters())\n",
    "    optimizer.zero_grad()\n",
    "    actor_loss, critic_loss = ac.compute_loss(states, actions, td_targets)\n",
    "    critic_loss.backward()\n",
    "    optimizer.step()\n",
    "    yield all(torch.all(new == old).item() for new, old in zip(ac.actor.parameters(), actor_params)), \\\n",
    "        'The weights of the actor network have changed after minimizing the critic_loss.'\n",
    "\n",
    "    yield None\n",
    "\n",
    "\n",
    "rl_tests.run_tests(test_actor_critic())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing select_actions()...\n",
      "0/3 tests passed!\n",
      "Test #1 failed: actions are incorrect (can only be tested if you use torch.multinomial)\n",
      "Test #2 failed: actions are incorrect (can only be tested if you use torch.multinomial)\n",
      "Test #3 failed: actions are incorrect (can only be tested if you use torch.multinomial)\n",
      "\n",
      "Testing compute_log_probs()...\n",
      "3/3 tests passed!\n",
      "\n",
      "Testing compute_td_targets()...\n",
      "0/3 tests passed!\n",
      "Test #1 failed: td_targets are incorrect (error = 3.4495322704315186)\n",
      "Test #2 failed: td_targets are incorrect (error = 9.697572708129883)\n",
      "Test #3 failed: td_targets are incorrect (error = 6.582981109619141)\n",
      "\n",
      "Testing compute_loss()...\n",
      "0/3 tests passed!\n",
      "Test #1 failed:\n",
      "actor_loss is incorrect (error = -0.5926303863525391)\n",
      "critic_loss is incorrect (error = 7.7692694664001465)\n",
      "gradients disabled for critic_loss\n",
      "Test #2 failed:\n",
      "actor_loss is incorrect (error = -12.202447891235352)\n",
      "critic_loss is incorrect (error = -12.429047584533691)\n",
      "gradients disabled for critic_loss\n",
      "Test #3 failed:\n",
      "actor_loss is incorrect (error = 5.646246910095215)\n",
      "critic_loss is incorrect (error = 2.1356465816497803)\n",
      "gradients disabled for critic_loss\n",
      "\n",
      "Testing gradients...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 108\u001B[0m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mall\u001B[39m(torch\u001B[38;5;241m.\u001B[39mall(new \u001B[38;5;241m==\u001B[39m old)\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mfor\u001B[39;00m new, old \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(ac\u001B[38;5;241m.\u001B[39mactor\u001B[38;5;241m.\u001B[39mparameters(), actor_params)), \\\n\u001B[1;32m    103\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe weights of the actor network have changed after minimizing the critic_loss.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 108\u001B[0m \u001B[43mrl_tests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_tests\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_actor_critic\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Uni/Reinforcement Learning/Exercises/Exercise 11/rl_tests.py:58\u001B[0m, in \u001B[0;36mrun_tests\u001B[0;34m(generator)\u001B[0m\n\u001B[1;32m     56\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m assertion:\n\u001B[1;32m     57\u001B[0m                 failed_msgs[test_index]\u001B[38;5;241m.\u001B[39mappend(msg)\n\u001B[0;32m---> 58\u001B[0m             cmd \u001B[38;5;241m=\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43massertion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     flush()\n",
      "Cell \u001B[0;32mIn[4], line 100\u001B[0m, in \u001B[0;36mtest_actor_critic\u001B[0;34m()\u001B[0m\n\u001B[1;32m     98\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     99\u001B[0m actor_loss, critic_loss \u001B[38;5;241m=\u001B[39m ac\u001B[38;5;241m.\u001B[39mcompute_loss(states, actions, td_targets)\n\u001B[0;32m--> 100\u001B[0m \u001B[43mcritic_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    101\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mall\u001B[39m(torch\u001B[38;5;241m.\u001B[39mall(new \u001B[38;5;241m==\u001B[39m old)\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mfor\u001B[39;00m new, old \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(ac\u001B[38;5;241m.\u001B[39mactor\u001B[38;5;241m.\u001B[39mparameters(), actor_params)), \\\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe weights of the actor network have changed after minimizing the critic_loss.\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/deep-learning/lib/python3.10/site-packages/torch/_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    580\u001B[0m     )\n\u001B[0;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/deep-learning/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/deep-learning/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We will evaluate the algorithm on the LunarLander environment.  \n",
    "Please read the documentation: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "If an error occurs, you might have to install the `box2d` environments first:  \n",
    "`pip install gymnasium[box2d]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(seed):\n",
    "    env_id = 'LunarLander-v3'\n",
    "    env = gym.make(env_id, render_mode='rgb_array')\n",
    "    env.reset(seed=seed)\n",
    "    return env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env(seed=42)\n",
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'sum of rewards: {reward_sum:.2f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the policy using advantage actor-critic.  \n",
    "We have provided some hyperparameters as a starting point, but the results are not optimal.  \n",
    "Feel free to change the hyperparameters and try to find a better solution.  \n",
    "You can also try to change the neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4       # learning rate\n",
    "gamma = 0.9        # discount factor\n",
    "num_envs = 16      # number of environment instances (= batch size)\n",
    "num_steps = 20000  # number of iterations\n",
    "\n",
    "# create the environment instances, initialize with different random seeds\n",
    "envs = [create_env(seed=i) for i in range(num_envs)]\n",
    "\n",
    "# create the actor-critic agent and the optimizer\n",
    "state_dim = envs[0].observation_space.shape[0]\n",
    "num_actions = envs[0].action_space.n\n",
    "ac = AdvantageActorCritic(state_dim, num_actions, gamma)\n",
    "optimizer = optim.Adam(ac.parameters(), lr=alpha)\n",
    "\n",
    "# reset all environment instances\n",
    "states = []\n",
    "for env in envs:\n",
    "    state, _ = env.reset()\n",
    "    states.append(state)\n",
    "\n",
    "# convert batch of initial states to tensor\n",
    "states = torch.as_tensor(np.array(states))\n",
    "\n",
    "# only for plotting\n",
    "reward_sums = torch.zeros(num_envs)\n",
    "episode_rewards = []\n",
    "\n",
    "for _ in range(num_steps):\n",
    "    ac.eval()  # switch to evaluation mode\n",
    "    # select batch of actions given the batch of states\n",
    "    actions = ac.select_actions(states)\n",
    "\n",
    "    # step all environment instances and store results in these lists:\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    terminations = []\n",
    "    truncations = []\n",
    "    for i in range(num_envs):\n",
    "        action = actions[i].item()\n",
    "        next_state, reward, terminated, truncated, _ = envs[i].step(action)\n",
    "        rewards.append(reward)\n",
    "        next_states.append(next_state)\n",
    "        terminations.append(terminated)\n",
    "        truncations.append(truncated)\n",
    "\n",
    "    # convert the lists to tensors\n",
    "    rewards = torch.as_tensor(np.array(rewards, dtype=np.float32))\n",
    "    next_states = torch.as_tensor(np.array(next_states))\n",
    "    terminations = torch.as_tensor(np.array(terminations))\n",
    "    truncations = torch.as_tensor(np.array(truncations))\n",
    "\n",
    "    # optimize the actor and critic networks\n",
    "    ac.train()  # switch to training mode\n",
    "    optimizer.zero_grad()\n",
    "    td_targets = ac.compute_td_targets(rewards, next_states, terminations)\n",
    "    actor_loss, critic_loss = ac.compute_loss(states, actions, td_targets)\n",
    "    # to minimize both losses simultaneously, just sum them up\n",
    "    total_loss = actor_loss + critic_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # necessary for next iteration\n",
    "    states = next_states\n",
    "\n",
    "    reward_sums += rewards\n",
    "    dones = terminations | truncations\n",
    "    if dones.any():\n",
    "        # reset the environment instances that were terminated or truncated\n",
    "        for i in range(num_envs):\n",
    "            if dones[i]:\n",
    "                state, _ = envs[i].reset()\n",
    "                states[i] = torch.as_tensor(state)\n",
    "        \n",
    "        # only for plotting\n",
    "        episode_rewards.extend(reward_sums[dones].tolist())\n",
    "        reward_sums[dones] = 0\n",
    "\n",
    "plt.title('LunarLander')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Sum of rewards')\n",
    "plt.plot(episode_rewards, c='C0', alpha=0.3)\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(20) / 20, mode='valid'), c='C0');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the learned policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "state, _ = env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "\n",
    "ac.eval()\n",
    "\n",
    "while True:\n",
    "    action = ac.select_actions(torch.as_tensor(state).unsqueeze(0)).item()\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'action: {action}, sum of rewards: {reward_sum:.2f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
