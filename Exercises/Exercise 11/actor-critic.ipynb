{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor-Critic\n",
    "\n",
    "**Policy Gradient with Deep Learning**:  \n",
    "In general, the update rule of policy gradient can be written as:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\, \\Phi_t \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\n",
    "$$\n",
    "where $\\theta$ is the weight vector, $\\alpha$ is the learning rate, and $\\Phi_t \\in \\mathbb{R}$ indicates how good it was to select the action $a_t$ in the state $s_t$.  \n",
    "But how can this applied to deep learning, where $\\pi_\\theta(a_t | s_t)$ is computed by a neural network with weights $\\theta$?  \n",
    "The solution is to write down a \"pseudo\" loss function that will result in the same gradients via backpropagation:\n",
    "$$\n",
    "\\mathcal{L}_\\text{PG}(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ -\\Phi_t \\log \\pi_\\theta(A_t | S_t) \\right]\n",
    "$$\n",
    "This means that we don't have to compute the score function anymore, but only the log-probabilities,  \n",
    "and the automatic differentiation library (e.g., PyTorch) will do the rest for us.\n",
    "\n",
    "**Advantage Actor-Critic**:  \n",
    "In the lecture we have seen that the high variance of policy gradients can be reduced with a critic (= learned value function).  \n",
    "Advantage actor-critic approximates the advantage function with the TD error and uses it for $\\Phi_t$, i.e.,\n",
    "$$\n",
    "\\Phi_t = \\underbrace{r_{t+1} + \\gamma \\hat{v}(s_{t+1}, w)}_\\text{TD target} - \\hat{v}(s_t, w)\n",
    "$$\n",
    "where $\\hat{v}$ is a learned value function with weights $w$. The value function can be trained with any VFA method.  \n",
    "We will consider the usual mean squared error between the TD target and the prediction (semi-gradient TD):\n",
    "$$\n",
    "\\mathcal{L}_\\text{VF}(w) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\text{sg}(\\underbrace{R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w)}_\\text{TD target}) - \\hat{v}(S_t, w) \\right]\n",
    "$$\n",
    "where $\\text{sg}$ is the stop-gradient operator that indicates that no gradients will be backpropagated at this point (= `detach()` in PyTorch).  \n",
    "Note that the TD target is used in both losses and only needs to be computed once.\n",
    "\n",
    "In summary, advantage actor-critic trains two neural networks:\n",
    "- *Actor*: The policy network that computes $\\pi_\\theta(a | s)$.\n",
    "- *Critic*: The value function network that computes $\\hat{v}(s, w)$.\n",
    "\n",
    "**Softmax Policy**:  \n",
    "We will again use a softmax policy, where the scores (also called \"logits\") are computed by the neural network,  \n",
    "which are then normalized to a probability distribution using the softmax function.\n",
    "\n",
    "More formally, the actor network $f_\\theta$ computes a vector of scores $y = f_\\theta(s) \\in \\mathbb{R}^M$, where $M$ is the number of actions.  \n",
    "Then the policy is computed with:\n",
    "$$\n",
    "\\pi_\\theta(a | s) = \\frac{\\exp(y_a)}{\\sum_{a'} \\exp(y_{a'})}\n",
    "$$\n",
    "This can be implemented efficiently using the `torch.softmax()` function.  \n",
    "(https://pytorch.org/docs/stable/generated/torch.softmax.html)\n",
    "\n",
    "For the loss function of the actor, we need to compute the logarithm of the policy:  \n",
    "$$\n",
    "\\log \\pi_\\theta(a | s) = y_a - \\log \\sum_{a'} \\exp(y_{a'})\n",
    "$$\n",
    "The first term selects the score of the selected action.  \n",
    "The second term can be implemented efficiently and numerically stable using the `torch.logsumexp()` function.  \n",
    "(https://pytorch.org/docs/stable/generated/torch.logsumexp.html)\n",
    "\n",
    "**Stochastic Gradient Descent**:  \n",
    "Deep neural networks are trained with stochastic gradient descent, where *batches* of states, actions, and rewards are used for each update.  \n",
    "For DQN we can use experience replay to create batches from old transitions that we have collected, since Q-learning is an off-policy method.  \n",
    "But in the case of on-policy policy gradient, we cannot reuse old transitions and always have to use new data from the environment.\n",
    "\n",
    "A common approach to obtain batches for the on-policy setting is to create *multiple instances* of the environment.  \n",
    "For example, we can create $16$ instances of the same environment and call the `step()` function for all of them at each iteration.  \n",
    "\n",
    "**Algorithm**:  \n",
    "To summarize, this is a rough description of the algorithm we will use:  \n",
    "- Create $N$ environment instances\n",
    "- Reset all environment instances to obtain a batch of $N$ states\n",
    "- Loop forever:  \n",
    "  - Select a batch of $N$ actions using the policy $\\pi_\\theta$\n",
    "  - Sample transitions from all environment instances, to obtain a batch of rewards and next states\n",
    "  - Update the actor and critic network by minimizing the losses $\\mathcal{L}_\\text{PG}$ and $\\mathcal{L}_\\text{VF}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Make sure that the files `rl_gui.py` and `rl_tests.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import rl_gui\n",
    "import rl_tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor-Critic Agent\n",
    "\n",
    "The class `AdvantageActorCritic` is a subclass of `torch.nn.Module`, which takes care of the automatic differentiation.  \n",
    "It contains both networks for the actor and the critic.  \n",
    "Your task is to finish the implementation.\n",
    "\n",
    "We provide two utility functions that might be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_at_index(x, indices):\n",
    "    # Extracts the elements from x at the specified indices, i.e.,\n",
    "    # result[i] = x[i, indices[i]] for all i.\n",
    "    # - x has shape (batch_size, d)\n",
    "    # - indices has shape (batch_size)\n",
    "    # - the result has shape (batch_size)\n",
    "    return x.gather(-1, indices.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "def sample_categorical(probs):\n",
    "    # Samples from a categorical distribution (e.g. with probabilities computed with softmax).\n",
    "    # - probs has shape (batch_size, d)\n",
    "    # - the result has shape (batch_size)\n",
    "    return torch.multinomial(probs, 1, replacement=True).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvantageActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, num_actions, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma  # discount factor\n",
    "\n",
    "        # actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "        # critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def select_actions(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # compute the scores for the given states\n",
    "        scores = self.actor(states)\n",
    "        # scores has shape (batch_size, num_actions)\n",
    "        #######################################################################\n",
    "        # TODO Convert the computed scores to probabilities with the softmax  #\n",
    "        # function and sample a batch of actions.                             #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        return actions\n",
    "\n",
    "    def compute_log_probs(self, states, actions):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # actions has shape (batch_size)\n",
    "        # compute the scores for the given states\n",
    "        scores = self.actor(states)\n",
    "        # scores has shape (batch_size, num_actions)\n",
    "        #######################################################################\n",
    "        # TODO Compute the log-probabilities as described above for the given #\n",
    "        # batches of states and actions. You can use one of the utility       #\n",
    "        # functions and torch.logsumexp().                                    #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        return log_probs\n",
    "\n",
    "    def compute_values(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # compute values for the batch of states using the critic network\n",
    "        values = self.critic(states).squeeze(-1)\n",
    "        # values has shape (batch_size)\n",
    "        return values\n",
    "\n",
    "    def compute_td_targets(self, rewards, next_states, terminations):\n",
    "        # next_states has shape (batch_size, state_dim)\n",
    "        # rewards and terminations have shape (batch_size)\n",
    "\n",
    "        gamma = self.gamma  # discount factor\n",
    "\n",
    "        # turn off gradient computation\n",
    "        with torch.no_grad():\n",
    "            #######################################################################\n",
    "            # TODO Compute the batch of TD targets, i.e., the reward plus the     #\n",
    "            # discounted value of the next state. Remember that the next value    #\n",
    "            # should not be used for terminal states.                             #\n",
    "            #######################################################################\n",
    "            \n",
    "            #######################################################################\n",
    "            # End of your code.                                                   #\n",
    "            #######################################################################\n",
    "\n",
    "        return td_target\n",
    "\n",
    "    def compute_loss(self, states, actions, td_targets):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # actions, rewards, next_states, and terminations have shape (batch_size)\n",
    "        #######################################################################\n",
    "        # TODO Compute the loss for the actor (L_PG) and the critic (L_VF) as #\n",
    "        # described above. The expectations are approximated by taking the    #\n",
    "        # average over the batch (i.e., use `x.mean()`).                      #\n",
    "        # IMPORTANT: Apply the stop-gradient operator to the advantages (phi) #\n",
    "        # by using `x.detach()`, since we don't want to backpropagate from    #\n",
    "        # the loss of the actor to the critic network.                        #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        # return both losses\n",
    "        return actor_loss, critic_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_actor_critic():\n",
    "    torch.manual_seed(42)\n",
    "    rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "    state_dim = 5\n",
    "    num_actions = 3\n",
    "    gamma = 0.8\n",
    "    ac = AdvantageActorCritic(state_dim, num_actions, gamma)\n",
    "    with torch.no_grad():\n",
    "        for layer in (ac.actor[0], ac.actor[2], ac.actor[4], ac.critic[0], ac.critic[2], ac.critic[4]):\n",
    "            mean = rng.uniform(-0.5, 0.5)\n",
    "            layer.weight[:] = torch.as_tensor(rng.normal(mean, 0.1, layer.weight.shape))\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    sample_states = lambda batch_size: torch.as_tensor(rng.standard_normal((batch_size, state_dim), dtype=np.float32))\n",
    "    sample_actions = lambda batch_size: torch.as_tensor(rng.choice(num_actions, batch_size))\n",
    "\n",
    "    yield 'select_actions()'\n",
    "    for expected_actions in [\n",
    "        [0, 1, 0, 2, 1, 0, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0]\n",
    "    ]:\n",
    "        batch_size = len(expected_actions)\n",
    "        states = sample_states(batch_size)\n",
    "        actions = ac.select_actions(states)\n",
    "        #print(actions)\n",
    "        #print(torch.as_tensor(expected_actions))\n",
    "        if (yield from rl_tests.check_torch_tensor(actions, 'actions', shape=(batch_size,), dtype=torch.long)):\n",
    "            yield torch.all(actions == torch.as_tensor(expected_actions)).item(), 'actions are incorrect (can only be tested if you use torch.multinomial)'\n",
    "        yield None\n",
    "\n",
    "    yield 'compute_log_probs()'\n",
    "    for expected_log_probs in [\n",
    "        [-1.37329102e-04, -1.10322797e+00, -6.56127930e-04],\n",
    "        [-1.14602625, -14.17874146, -44.21386719,   0.00000000,  -1.09861231],\n",
    "        [-1.52587891e-05, -1.09861231e+00]\n",
    "    ]:\n",
    "        batch_size = len(expected_log_probs)\n",
    "        expected_log_probs = torch.as_tensor(expected_log_probs)\n",
    "        states = sample_states(batch_size)\n",
    "        actions = sample_actions(batch_size)\n",
    "        log_probs = ac.compute_log_probs(states, actions)\n",
    "        if (yield from rl_tests.check_torch_tensor(log_probs, 'log_probs', shape=(batch_size,), dtype=torch.float32)):\n",
    "            yield torch.allclose(log_probs, torch.as_tensor(expected_log_probs)), f'log_probs are incorrect (error = {torch.sum(torch.abs(log_probs - expected_log_probs)).item()})'\n",
    "        yield None\n",
    "\n",
    "    yield 'compute_td_targets()'\n",
    "    for expected_td_targets in [\n",
    "        [-1.1166, -0.3763,  0.1678, -0.1645],\n",
    "        [-1.0284, -0.8361,  1.0379, -1.6344,  1.1361],\n",
    "        [-0.9384,  0.2974, -1.0340, -0.8565, -0.1211, -0.9041, -0.6611]\n",
    "    ]:\n",
    "        batch_size = len(expected_td_targets)\n",
    "        expected_td_targets = torch.as_tensor(expected_td_targets)\n",
    "        rewards = torch.randn(batch_size, dtype=torch.float32)\n",
    "        next_states = sample_states(batch_size)\n",
    "        terminations = torch.rand(batch_size) < 0.5\n",
    "        td_targets = ac.compute_td_targets(rewards, next_states, terminations)\n",
    "        if (yield from rl_tests.check_torch_tensor(td_targets, 'td_targets', shape=(batch_size,), dtype=torch.float32)):\n",
    "            yield torch.allclose(td_targets, torch.as_tensor(expected_td_targets), atol=1e-04), f'td_targets are incorrect (error = {torch.sum(torch.abs(td_targets - expected_td_targets)).item()})'\n",
    "        yield None\n",
    "\n",
    "    yield 'compute_loss()'\n",
    "    for batch_size, expected_actor_loss, expected_critic_loss in [\n",
    "        (4, 8.9347, 0.5728),\n",
    "        (8, 0.2931, 0.5197),\n",
    "        (12, -2.5311, 0.9795)\n",
    "    ]:\n",
    "        states = sample_states(batch_size)\n",
    "        actions = sample_actions(batch_size)\n",
    "        td_targets = torch.randn(batch_size, dtype=torch.float32)\n",
    "        actor_loss, critic_loss = ac.compute_loss(states, actions, td_targets)\n",
    "        if (yield from rl_tests.check_torch_tensor(actor_loss, 'actor_loss', shape=tuple(), dtype=torch.float32)):\n",
    "            expected_actor_loss = torch.tensor(expected_actor_loss)\n",
    "            yield torch.isclose(actor_loss, expected_actor_loss, atol=1e-04), f'actor_loss is incorrect (error = {(actor_loss - expected_actor_loss).item()})'\n",
    "            yield actor_loss.requires_grad, 'gradients disabled for actor_loss'\n",
    "        if (yield from rl_tests.check_torch_tensor(critic_loss, 'critic_loss', shape=tuple(), dtype=torch.float32)):\n",
    "            expected_critic_loss = torch.tensor(expected_critic_loss)\n",
    "            yield torch.isclose(critic_loss, expected_critic_loss, atol=1e-04), f'critic_loss is incorrect (error = {(actor_loss - expected_critic_loss).item()})'\n",
    "            yield critic_loss.requires_grad, 'gradients disabled for critic_loss'\n",
    "        yield None\n",
    "\n",
    "    yield 'gradients'\n",
    "    optimizer = optim.SGD(ac.parameters(), lr=0.1)\n",
    "    states = sample_states(batch_size)\n",
    "    actions = sample_actions(batch_size)\n",
    "    td_targets = torch.randn(batch_size, dtype=torch.float32)\n",
    "\n",
    "    critic_params = tuple(param.data.clone() for param in ac.critic.parameters())\n",
    "    optimizer.zero_grad()\n",
    "    actor_loss, critic_loss = ac.compute_loss(states, actions, td_targets)\n",
    "    actor_loss.backward()\n",
    "    optimizer.step()\n",
    "    yield all(torch.all(new == old).item() for new, old in zip(ac.critic.parameters(), critic_params)), \\\n",
    "        'The weights of the critic network have changed after minimizing the actor_loss. Did you use x.detach() on the advantages?'\n",
    "\n",
    "    actor_params = tuple(param.data.clone() for param in ac.actor.parameters())\n",
    "    optimizer.zero_grad()\n",
    "    actor_loss, critic_loss = ac.compute_loss(states, actions, td_targets)\n",
    "    critic_loss.backward()\n",
    "    optimizer.step()\n",
    "    yield all(torch.all(new == old).item() for new, old in zip(ac.actor.parameters(), actor_params)), \\\n",
    "        'The weights of the actor network have changed after minimizing the critic_loss.'\n",
    "\n",
    "    yield None\n",
    "\n",
    "\n",
    "rl_tests.run_tests(test_actor_critic())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We will evaluate the algorithm on the LunarLander environment.  \n",
    "Please read the documentation: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "If an error occurs, you might have to install the `box2d` environments first:  \n",
    "`pip install gymnasium[box2d]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(seed):\n",
    "    env_id = 'LunarLander-v3'\n",
    "    env = gym.make(env_id, render_mode='rgb_array')\n",
    "    env.reset(seed=seed)\n",
    "    return env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env(seed=42)\n",
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'sum of rewards: {reward_sum:.2f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the policy using advantage actor-critic.  \n",
    "We have provided some hyperparameters as a starting point, but the results are not optimal.  \n",
    "Feel free to change the hyperparameters and try to find a better solution.  \n",
    "You can also try to change the neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-4       # learning rate\n",
    "gamma = 0.9        # discount factor\n",
    "num_envs = 16      # number of environment instances (= batch size)\n",
    "num_steps = 20000  # number of iterations\n",
    "\n",
    "# create the environment instances, initialize with different random seeds\n",
    "envs = [create_env(seed=i) for i in range(num_envs)]\n",
    "\n",
    "# create the actor-critic agent and the optimizer\n",
    "state_dim = envs[0].observation_space.shape[0]\n",
    "num_actions = envs[0].action_space.n\n",
    "ac = AdvantageActorCritic(state_dim, num_actions, gamma)\n",
    "optimizer = optim.Adam(ac.parameters(), lr=alpha)\n",
    "\n",
    "# reset all environment instances\n",
    "states = []\n",
    "for env in envs:\n",
    "    state, _ = env.reset()\n",
    "    states.append(state)\n",
    "\n",
    "# convert batch of initial states to tensor\n",
    "states = torch.as_tensor(np.array(states))\n",
    "\n",
    "# only for plotting\n",
    "reward_sums = torch.zeros(num_envs)\n",
    "episode_rewards = []\n",
    "\n",
    "for _ in range(num_steps):\n",
    "    ac.eval()  # switch to evaluation mode\n",
    "    # select batch of actions given the batch of states\n",
    "    actions = ac.select_actions(states)\n",
    "\n",
    "    # step all environment instances and store results in these lists:\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    terminations = []\n",
    "    truncations = []\n",
    "    for i in range(num_envs):\n",
    "        action = actions[i].item()\n",
    "        next_state, reward, terminated, truncated, _ = envs[i].step(action)\n",
    "        rewards.append(reward)\n",
    "        next_states.append(next_state)\n",
    "        terminations.append(terminated)\n",
    "        truncations.append(truncated)\n",
    "\n",
    "    # convert the lists to tensors\n",
    "    rewards = torch.as_tensor(np.array(rewards, dtype=np.float32))\n",
    "    next_states = torch.as_tensor(np.array(next_states))\n",
    "    terminations = torch.as_tensor(np.array(terminations))\n",
    "    truncations = torch.as_tensor(np.array(truncations))\n",
    "\n",
    "    # optimize the actor and critic networks\n",
    "    ac.train()  # switch to training mode\n",
    "    optimizer.zero_grad()\n",
    "    td_targets = ac.compute_td_targets(rewards, next_states, terminations)\n",
    "    actor_loss, critic_loss = ac.compute_loss(states, actions, td_targets)\n",
    "    # to minimize both losses simultaneously, just sum them up\n",
    "    total_loss = actor_loss + critic_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # necessary for next iteration\n",
    "    states = next_states\n",
    "\n",
    "    reward_sums += rewards\n",
    "    dones = terminations | truncations\n",
    "    if dones.any():\n",
    "        # reset the environment instances that were terminated or truncated\n",
    "        for i in range(num_envs):\n",
    "            if dones[i]:\n",
    "                state, _ = envs[i].reset()\n",
    "                states[i] = torch.as_tensor(state)\n",
    "        \n",
    "        # only for plotting\n",
    "        episode_rewards.extend(reward_sums[dones].tolist())\n",
    "        reward_sums[dones] = 0\n",
    "\n",
    "plt.title('LunarLander')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Sum of rewards')\n",
    "plt.plot(episode_rewards, c='C0', alpha=0.3)\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(20) / 20, mode='valid'), c='C0');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the learned policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "state, _ = env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "\n",
    "ac.eval()\n",
    "\n",
    "while True:\n",
    "    action = ac.select_actions(torch.as_tensor(state).unsqueeze(0)).item()\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'action: {action}, sum of rewards: {reward_sum:.2f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
