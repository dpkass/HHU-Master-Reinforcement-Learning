{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić\n",
    "\n",
    "### Multi-step double DQN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch needs to be installed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import rl_gui\n",
    "import rl_tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short PyTorch intro\n",
    "\n",
    "In this notebook we will use the PyTorch library to implement deep Q-networks.  \n",
    "The main advantages of PyTorch over NumPy are fast parallel computing on GPUs and automatic differentiation, i.e., gradients are computed for us automatically.\n",
    "\n",
    "In this notebook we will not make use of a GPU, since the network is fairly small and computation on the CPU might even be faster.\n",
    "\n",
    "PyTorch uses *tensors*, which are very similar to NumPy arrays, but they can be stored on a specific device (e.g., a GPU).  \n",
    "The following code creates a tensor of shape `(8,3)`, i.e., a matrix in $\\mathbb{R}^{8 \\times 3}$, filled with zeros and stores it in the main memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((8, 3))\n",
    "print(x)\n",
    "print()\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational efficiency (and for more stable training), we will work with *batches* of data.  \n",
    "For example, instead of updating the action value function for a single state, we update multiple states at once.  \n",
    "E.g., if the *batch size* is 64, then the tensors for a single update might have the following shapes:\n",
    "- `states`: `(64,5)`, i.e., the tensor contains 64 states and each state is a 5-dimensional vector\n",
    "- `actions`: `(64)`, i.e., the tensor contains 64 actions\n",
    "- `rewards`: `(64)`, i.e., the tensor contains 64 rewards\n",
    "\n",
    "If tensors have the same batch size, we can work with them as if they are single values.  \n",
    "E.g., if we have `rewards_0` with shape `(64)` and `rewards_1` with shape `(64)`, we can compute:  \n",
    "\n",
    "`rewards_0 + gamma * rewards1`  \n",
    "\n",
    "which will be a new tensor of shape `(64)`.\n",
    "\n",
    "We already provide the code that takes care of automatic differentiation and optimization,  \n",
    "so it should be sufficient if you understand the concept of tensors and batches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Networks (DQN)\n",
    "  \n",
    "As seen in the lecture, DQN learns an action-value function by minimizing the following mean squared error:\n",
    "$$\n",
    "\\mathbb{E}_{s_t, a_t, r_{t+1}, s_{t+1}} [ (y_t - \\hat{q}(s_t, a_t, w))^2 ],\n",
    "$$\n",
    "where $w$ are the parameters of the \"online\" network, $w^-$ are the parameters of the fixed target network, and\n",
    "$y_t$ is the update target, which is defined as\n",
    "$$\n",
    "y_t = r_{t+1} + \\gamma \\max_a \\hat{q}(s_{t+1}, a, w^-).\n",
    "$$\n",
    "\n",
    "**Double DQN:**  \n",
    "The idea of double DQN (https://arxiv.org/abs/1509.06461) is to apply double Q-learning to the deep Q-network.  \n",
    "The simplest way of doing this is to use the online network to select actions, but the fixed target network to compute the next action value.  \n",
    "This leads to the update target\n",
    "$$\n",
    "y_t = r_{t+1} + \\gamma \\hat{q}(s_{t+1}, \\arg\\max_a \\hat{q}(s_{t+1}, a, w), w^-).\n",
    "$$\n",
    "\n",
    "**Multi-step DQN:**  \n",
    "The DQN can also be extended to $n$-step returns (https://arxiv.org/abs/1710.02298), which leads to the following update target:\n",
    "$$\n",
    "y_t = \\sum_{k=0}^{n-1} \\gamma^k r_{t+k+1} + \\gamma^n \\max_a \\hat{q}(s_{t+n}, a, w^-)\n",
    "$$\n",
    "\n",
    "**Multi-step double DQN:**  \n",
    "By combining both approaches we get the following update target, which we will use in this exercise:\n",
    "$$\n",
    "\\sum_{k=0}^{n-1} \\gamma^k r_{t+k+1} + \\gamma^n \\hat{q}(s_{t+n}, \\arg\\max_a \\hat{q}(s_{t+n}, a, w), w^-)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay Memory\n",
    "DQN stores transitions in a replay memory. Batches of transitions are sampled from the memory and used to update the DQN.  \n",
    "We have already implemented most of the replay memory below. Your task is to extend the functionality to sample multiple steps at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity, rng):\n",
    "        # create a queue that removes old transitions when capacity is reached\n",
    "        self.transitions = collections.deque([], maxlen=capacity)\n",
    "\n",
    "        # random number generator used for sampling batches\n",
    "        self.rng = rng\n",
    "\n",
    "    def append(self, transition):\n",
    "        # append a transition (a tuple) to the queue\n",
    "        self.transitions.append(transition)\n",
    "\n",
    "    def sample(self, batch_size, n_steps):\n",
    "        # randomly sample a list of indices\n",
    "        idx = self.rng.choice(len(self.transitions) - n_steps + 1, batch_size, replace=False)\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO Modify the code to support multi-step sampling. The parameter  #\n",
    "        # `n_steps` specifies how many steps should be sampled. The return    #\n",
    "        # value `sequence` should be a list of batches for each time step.    #\n",
    "        # This means that the result of the existing code below should be     #\n",
    "        # stored in sequence[0], and you have to add the following steps to   #\n",
    "        # the list (with the start indices from `idx`, but with an offset).   #\n",
    "        #######################################################################\n",
    "        # select the transitions using the indices\n",
    "        transitions = [self.transitions[i + j] for i in idx]\n",
    "\n",
    "        # convert the list of transitions into multiple batches, one for each modality (states, actions, rewards, ...), i.e.\n",
    "        # transitions: (s_1, a_1, r_1, ...), ..., (s_n, a_n, r_n, ...)\n",
    "        # get converted into\n",
    "        # batches: (s_1, ..., s_n), (a_1, ..., a_n), (r_1, ..., r_n), ...\n",
    "        # they also get converted into PyTorch tensors\n",
    "        batches = tuple(torch.as_tensor(np.array(batch)) for batch in zip(*transitions))\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        return sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_replay_memory():\n",
    "    yield 'sample()'\n",
    "\n",
    "    rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "    memory = ReplayMemory(capacity=10, rng=rng)\n",
    "\n",
    "    transitions = [\n",
    "        (1, 2, 3),\n",
    "        (4, 5, 6),\n",
    "        (7, 8, 9),\n",
    "        (10, 11, 12),\n",
    "        (13, 14, 15),\n",
    "        (16, 17, 18)\n",
    "    ]\n",
    "\n",
    "    for transition in transitions:\n",
    "        memory.append(transition)\n",
    "\n",
    "    test_data = [\n",
    "        (4, 3, {(0, 0): [1, 4, 7], (1, 2): [11, 14, 17], (2, 3): [6, 9, 12]}),\n",
    "        (2, 2, {(0, 1): [1, 4], (2, 0): [12, 15]})\n",
    "    ]\n",
    "\n",
    "    for batch_size, n_steps, samples in test_data:\n",
    "        sequence = memory.sample(batch_size, n_steps)\n",
    "        yield isinstance(sequence, (tuple, list)), 'sequence must be a tuple or list'\n",
    "        yield len(sequence) == n_steps, 'Length of sequence is incorrect'\n",
    "\n",
    "        for batches in sequence:\n",
    "            yield isinstance(batches, (tuple, list)), 'Each value of sequence must be a tuple or list (containing the batches)'\n",
    "            for batch in batches:\n",
    "                yield torch.is_tensor(batch), 'Each batch must be a tensor'\n",
    "                yield batch.shape[0] == batch_size, 'Batch size is incorrect'\n",
    "        \n",
    "        for (modality, index), values in samples.items():\n",
    "            for step in range(n_steps):\n",
    "                yield sequence[step][modality][index] == values[step], 'Tensor contains incorrect value'\n",
    "\n",
    "        yield None\n",
    "\n",
    "\n",
    "rl_tests.run_tests(test_replay_memory())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Q-network\n",
    "\n",
    "The class `MultiStepDDQN` is a subclass of `torch.nn.Module`, which takes care of the automatic differentiation.  \n",
    "Your task is to finish the implementation of `compute_q()`, `compute_max_q()` and `compute_loss()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepDDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        super().__init__()\n",
    "        # create a simple neural network with two fully-connected layers\n",
    "        # and a ReLU nonlinearity\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "    def compute_q(self, states, actions):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # actions has shape (batch_size)\n",
    "\n",
    "        # compute q[s], which has shape (batch_size, num_actions)\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO Implement the computation of q(s,a), given states and actions   #\n",
    "        # using self.network. The neural network takes as input a batch of     #\n",
    "        # states and produces a tensor of size (batch_size, num_actions)       #\n",
    "        # HINT: You can use the function torch.gather with inputs              #\n",
    "        # q_all and actions                                                    #\n",
    "        ########################################################################\n",
    "\n",
    "        # select q[s,a], which has shape (batch_size); use torch.gather\n",
    "\n",
    "        ########################################################################\n",
    "        # End of your code.                                                    #\n",
    "        ########################################################################\n",
    "\n",
    "        return q\n",
    "\n",
    "    def compute_max_q(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "\n",
    "        # compute q[s], which has shape (batch_size, num_actions)\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO Implement the computation of max_a' q(s,a'), given states       #\n",
    "        # using self.network. The neural network takes as input a batch of     #\n",
    "        # states and produces a tensor of size (batch_size, num_actions)       #\n",
    "        ########################################################################\n",
    "\n",
    "        # select max_a' q[s,a'], which has shape (batch_size)\n",
    "\n",
    "        ########################################################################\n",
    "        # End of your code.                                                    #\n",
    "        ########################################################################\n",
    "\n",
    "        return max_q\n",
    "\n",
    "    def compute_arg_max(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "\n",
    "        # compute q[s], which has shape (batch_size, num_actions)\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        # select argmax_a' q[s,a'], which has shape (batch_size)\n",
    "        actions = q_all.argmax(dim=1)\n",
    "        return actions\n",
    "\n",
    "    def compute_loss(self, target_dqn, sequence, gamma):\n",
    "        # sequence contains a list of batches from the replay memory\n",
    "\n",
    "        # the number of steps is the length of the sequence\n",
    "        n_steps = len(sequence)\n",
    "\n",
    "        # sequence[0] contains the states and actions that should be updated\n",
    "        states_0, actions_0, rewards_0, terminations_0, next_states_0 = sequence[0]\n",
    "\n",
    "        # sequence[n_steps - 1] contains the next states that are used for bootstrapping\n",
    "        states_n, actions_n, rewards_n, terminations_n, next_states_n = sequence[n_steps - 1]\n",
    "\n",
    "        # turn off gradient computation\n",
    "        with torch.no_grad():\n",
    "            #######################################################################\n",
    "            # TODO Implement the multi-step DDQN targets as described above.      #\n",
    "            # You can see how we obtained the batches for the first and last time #\n",
    "            # step above. You need to do the same for all n steps in order to     #\n",
    "            # calculate the targets.                                              #\n",
    "            #                                                                     #\n",
    "            # Hint #1: The implementation for the normal 1-step DQN (without      #\n",
    "            # double Q-learning) would look like this:                            #\n",
    "            # max_q = target_dqn.compute_max_q(next_states_n)                     #\n",
    "            # targets = rewards_0 + gamma * (~terminations) * max_q               #\n",
    "            # Hint #2: Iterate backwards.                                         #\n",
    "            #######################################################################\n",
    "            \n",
    "            #######################################################################\n",
    "            # End of your code.                                                   #\n",
    "            #######################################################################\n",
    "\n",
    "        # compute predictions q[s,a]\n",
    "        q = self.compute_q(states_0, actions_0)\n",
    "\n",
    "        # compute mean squared error between q[s,a] and targets\n",
    "        loss = torch.mean((q - targets.detach()) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multi_ddqn():\n",
    "    yield 'compute_loss()'\n",
    "\n",
    "    rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "    state_dim = 5\n",
    "    num_actions = 3\n",
    "    ddqn = MultiStepDDQN(state_dim, num_actions)\n",
    "    with torch.no_grad():\n",
    "        layer1, layer2 = ddqn.network[0], ddqn.network[2]\n",
    "        layer1.weight[:] = torch.as_tensor(rng.normal(0, 0.01, layer1.weight.shape))\n",
    "        layer2.weight[:] = torch.as_tensor(rng.normal(0, 0.01, layer2.weight.shape))\n",
    "        nn.init.zeros_(layer1.bias)\n",
    "        nn.init.zeros_(layer2.bias)\n",
    "    \n",
    "    memory = ReplayMemory(20, rng)\n",
    "    state = rng.standard_normal(state_dim, dtype=np.float32)\n",
    "    for _ in range(100):\n",
    "        action = rng.integers(num_actions)\n",
    "        reward = rng.standard_normal(dtype=np.float32)\n",
    "        terminated = rng.uniform(0.0, 1.0) < 0.2\n",
    "        next_state = rng.standard_normal(state_dim, dtype=np.float32)\n",
    "        memory.append((state, action, reward, terminated, next_state))\n",
    "        if terminated:\n",
    "            state = rng.standard_normal(state_dim, dtype=np.float32)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    gamma = 0.8\n",
    "    for batch_size, n_steps, expected_loss in [(6, 3, 2.01768165), (16, 0, 1.17322925), (2, 8, 2.35127176)]:\n",
    "        sequence = memory.sample(batch_size=6, n_steps=3)\n",
    "        loss = ddqn.compute_loss(ddqn, sequence, gamma).item()\n",
    "        yield np.isclose(loss, expected_loss), f'Loss is incorrect (error = {abs(expected_loss - loss)})'\n",
    "\n",
    "    yield None\n",
    "\n",
    "\n",
    "rl_tests.run_tests(test_multi_ddqn())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole environment\n",
    "\n",
    "We will evaluate the agent on the CartPole environment.  \n",
    "You can read more about it here: https://www.gymlibrary.dev/environments/classic_control/cart_pole/  \n",
    "(we use the `-v0` version)\n",
    "\n",
    "Define a function to create the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(seed):\n",
    "    env_id = f'CartPole-v0'\n",
    "    env = gym.make(env_id, render_mode='rgb_array')\n",
    "    env.reset(seed=seed)\n",
    "    return env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env(seed=42)\n",
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'sum of rewards: {reward_sum}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We already implemented the training code below.  \n",
    "The agent should reach a score of 200 at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random number generators\n",
    "seed = 1\n",
    "rng = np.random.default_rng(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "num_steps = 30000        # number of steps in the environment\n",
    "batch_size = 64          # number of transitions in a batch\n",
    "replay_capacity = 1000   # number of transitions that are stored in memory\n",
    "gamma = 0.99             # discount factor\n",
    "n_steps = 3              # number of steps for multi-step target\n",
    "learning_rate = 0.001    # learning rate\n",
    "target_interval = 100    # synchronize the target network after this number of steps\n",
    "\n",
    "# decrease the epsilon-greedy probability linearly\n",
    "epsilon_start = 1.0     # start value\n",
    "epsilon_end = 0.05      # end value\n",
    "epsilon_steps = 5000    # number of steps to reach the end value\n",
    "\n",
    "# create the environment and replay memory\n",
    "env = create_env(seed)\n",
    "memory = ReplayMemory(replay_capacity, rng)\n",
    "\n",
    "# create the deep Q-network and optimizer\n",
    "state_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "dqn = MultiStepDDQN(state_dim, num_actions)\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=learning_rate)\n",
    "\n",
    "# create the target Q-network\n",
    "target_dqn = copy.deepcopy(dqn)\n",
    "\n",
    "# store values for learning curves\n",
    "loss_steps = []\n",
    "loss_history = []\n",
    "reward_steps = []\n",
    "reward_history = []\n",
    "episode_i = 0\n",
    "reward_sum = 0.0\n",
    "\n",
    "# main training loop\n",
    "state, _ = env.reset(seed=seed)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    update_plot = False\n",
    "\n",
    "    # compute the epsilon-greedy probability\n",
    "    epsilon = epsilon_start + (epsilon_end - epsilon_start) * min(step / epsilon_steps, 1.0)\n",
    "\n",
    "    # select an action\n",
    "    if rng.random() < epsilon:\n",
    "        action = rng.choice(num_actions)\n",
    "    else:\n",
    "        dqn.eval()  # switch to evaluation mode\n",
    "        # convert state to tensor, need to call unsqueeze(0) for a batch size of 1\n",
    "        action = dqn.compute_arg_max(torch.as_tensor(state).unsqueeze(0)).item()\n",
    "\n",
    "    # execute the action\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    # convert reward to float32 (should be faster)\n",
    "    reward = np.float32(reward)\n",
    "\n",
    "    # add the transition to replay memory\n",
    "    transition = (state, action, reward, terminated, next_state)\n",
    "    memory.append(transition)\n",
    "    reward_sum += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        reward_steps.append(episode_i)\n",
    "        reward_history.append(reward_sum)\n",
    "        episode_i += 1\n",
    "        state, _ = env.reset()\n",
    "        reward_sum = 0.0\n",
    "        update_plot = True\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    # check if enough transitions in replay memory\n",
    "    if step >= batch_size + n_steps:\n",
    "        # sample a batch of transitions from the replay memory\n",
    "        sequence = memory.sample(batch_size, n_steps)\n",
    "        # minimize the loss function using SGD\n",
    "        dqn.train()  # switch to training mode\n",
    "        loss = dqn.compute_loss(target_dqn, sequence, gamma)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            loss_steps.append(step)\n",
    "            loss_history.append(loss.item())\n",
    "            update_plot = True\n",
    "\n",
    "    # synchronize the target network if necessary\n",
    "    if step > 0 and step % target_interval == 0:\n",
    "        target_dqn = copy.deepcopy(dqn)\n",
    "\n",
    "    # plot curves for the loss and sum of rewards\n",
    "    if update_plot:\n",
    "        clear_output(wait=True)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "        axes[0].set_xlabel('step')\n",
    "        axes[0].set_ylabel('loss')\n",
    "        axes[0].plot(loss_steps, loss_history)\n",
    "        axes[1].set_xlabel('episode')\n",
    "        axes[1].set_ylabel('sum of rewards')\n",
    "        axes[1].plot(reward_steps, reward_history)\n",
    "        plt.subplots_adjust(wspace=0.2)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the trained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "with torch.no_grad():\n",
    "    dqn.eval()\n",
    "    state, _ = env.reset()\n",
    "    render()\n",
    "    reward_sum = 0.0\n",
    "\n",
    "    while True:\n",
    "        state = torch.as_tensor(state).unsqueeze(0)\n",
    "        action = dqn.compute_arg_max(state).item()\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        render(f'action: {action}, sum of rewards: {reward_sum:.2f}')\n",
    "        if terminated or truncated:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
