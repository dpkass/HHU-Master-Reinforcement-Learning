{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD($\\lambda$) value function approximation\n",
    "\n",
    "This notebook is a follow-up of last week. Last week we implemented semi-gradient TD(0) with linear value function approximation. In this notebook, we will add TD($\\lambda$) and compare it to TD(0).\n",
    "\n",
    "As last week, we will represent the value function by a linear combination of features:\n",
    "$$\n",
    "\\hat{v}(s,w) = x(s)^\\top w = \\sum_{j=1}^d x(s)_j \\, w_j\n",
    "$$\n",
    "where $s$ is a state, $x(s) \\in \\mathbb{R}^d$ is a feature vector of the state, and $w \\in \\mathbb{R}^d$ is a weight vector.\n",
    "\n",
    "As a recap, the update rule for semi-gradient TD(0) prediction is given by\n",
    "$$\n",
    "w \\leftarrow w + \\alpha (\\underbrace{r_{t+1} + \\gamma \\hat{v}(s_{t+1}, w) - \\hat{v}(s_t, w)}_{\\text{TD error}}) \\nabla_w \\hat{v}(s_t, w)\n",
    "$$\n",
    "where $\\alpha > 0$ is the learning rate and $\\nabla_w \\hat{v}(s_t, w)$ is the gradient of the value function with respect to the weight vector.  \n",
    "For a linear value function the gradient is equal to $\\nabla_w \\hat{v}(s_t, w) = x(s)$.\n",
    "\n",
    "To implement (backward-view) semi-gradient TD($\\lambda$), we also keep an eligibility vector $E \\in \\mathbb{R}^d$ in addition to the weight vector.  \n",
    "The update rule updates the eligibility vector and the weight vector:\n",
    "$$\\begin{aligned}\n",
    "E & \\leftarrow \\gamma \\lambda E + \\nabla_w \\hat{v}(s_t, w) \\\\\n",
    "w & \\leftarrow w + \\alpha (\\underbrace{r_{t+1} + \\gamma \\hat{v}(s_{t+1}, w) - \\hat{v}(s_t, w)}_{\\text{TD error}}) E\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Make sure that the files `rl_env.py` and `rl_tests.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rl_env\n",
    "import rl_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "\n",
    "We will work with tabular environments with state space $\\mathcal{S} = \\{0, 1, \\ldots, N\\}$,  \n",
    "where we assume $0$ to be the terminal state and $N$ is the number of non-terminal states.\n",
    "\n",
    "To apply linear value function approximation, we need a way to convert an integer state $s \\in \\mathcal{S}$ into a real-valued feature vector $x(s) \\in \\mathbb{R}^d$.  \n",
    "Since the value of terminal states is always zero, we don't need a feature vector for terminal state $s = 0$.  \n",
    "We will consider three types of feature vectors:\n",
    "\n",
    "**1. One-hot features** (aka table lookup features): $x(s) = \\begin{bmatrix} [s = 1], [s = 2], \\ldots, [s = N] \\end{bmatrix}^\\top$.  \n",
    "In other words, $x(s)$ is an $N$-dimensional vector with a 1 for state $s$ and 0s everywhere else.\n",
    "\n",
    "**2. State aggregation features**: $x(s) = \\begin{bmatrix} [1 \\le s \\le k], [k + 1 \\le s \\le 2k], \\ldots, [N-k+1 \\le s \\le N] \\end{bmatrix}^\\top$.  \n",
    "In other words, we aggregate states into groups and create a one-hot vector. The size of each group is $k$.\n",
    "\n",
    "**3. State normalization features**: $x(s) = \\begin{bmatrix} (s - 1) / (N - 1) - 0.5 \\end{bmatrix}^\\top$.  \n",
    "In other words, we convert the integer into a real value between $-0.5$ and $0.5$ and create a 1-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different feature functions were already implemented last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotFeatures:\n",
    "\n",
    "    def __init__(self, observation_space):\n",
    "        if not isinstance(observation_space, gym.spaces.Discrete):\n",
    "            raise ValueError('Observation space must be discrete')\n",
    "\n",
    "        # number of non-terminal states\n",
    "        self.num_states = observation_space.n - 1\n",
    "\n",
    "    def __call__(self, state):\n",
    "\n",
    "        x = np.zeros(self.num_states)\n",
    "        x[state - 1] = 1\n",
    "\n",
    "        return x.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "class StateAggregationFeatures:\n",
    "\n",
    "    def __init__(self, observation_space, group_size):\n",
    "        # state 0 is assumed to be the terminal state\n",
    "\n",
    "        if not isinstance(observation_space, gym.spaces.Discrete):\n",
    "            raise ValueError('Observation space must be discrete')\n",
    "        if (observation_space.n - 1) % group_size != 0:\n",
    "            raise ValueError('Number of non-terminal states must be divisible by group_size')\n",
    "\n",
    "        self.group_size = group_size\n",
    "\n",
    "        # number of non-terminal states\n",
    "        num_states = observation_space.n - 1\n",
    "        self.num_groups = num_states // group_size\n",
    "\n",
    "    def __call__(self, state):\n",
    "\n",
    "        x = np.zeros(self.num_groups)\n",
    "        x[(state - 1) // self.group_size] = 1\n",
    "\n",
    "        return x.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "class StateNormalizationFeatures:\n",
    "\n",
    "    def __init__(self, observation_space):\n",
    "        if not isinstance(observation_space, gym.spaces.Discrete):\n",
    "            raise ValueError('Observation space must be discrete')\n",
    "\n",
    "        # number of non-terminal states\n",
    "        self.num_states = observation_space.n - 1\n",
    "    \n",
    "    def __call__(self, state):\n",
    "\n",
    "        x = -0.5 + (state - 1) / (self.num_states -1)\n",
    "        x = np.array([x])\n",
    "\n",
    "        return x.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value functions\n",
    "\n",
    "We will use the following base class for state value functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction:\n",
    "\n",
    "    def reset(self):\n",
    "        # reset the value function, e.g. reset the weight vector\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def evaluate(self, state):\n",
    "        # compute the value of a state\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update(self, state, target):\n",
    "        # update the value of a state toward a given target\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class implements linear value functions and supports TD(0) and TD($\\lambda$).  \n",
    "It stores the weight vector and an eligibility vector, if necessary.  \n",
    "The learning rate `alpha` is also part of the value function and it will be used in the `update()` method.  \n",
    "The parameter `feature_fn` is a function that computes a feature vector for a given state.\n",
    "\n",
    "Your task is to implement the update function, which takes care of TD(0) and TD($\\lambda$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearValueFunction(ValueFunction):\n",
    "\n",
    "    def __init__(self, observation_space, alpha, gamma, lmbda, feature_fn):\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.lmbda = lmbda  # value for TD(lambda)\n",
    "\n",
    "        # function that converts a state into a feature vector\n",
    "        self.feature_fn = feature_fn\n",
    "\n",
    "        # determine the size of the feature vectors\n",
    "        # sample a random state and convert it to a feature vector\n",
    "        sample_features = feature_fn(observation_space.sample())\n",
    "        if not (isinstance(sample_features, np.ndarray) and len(sample_features.shape) == 1):\n",
    "            raise ValueError('Features must be real-valued vectors')\n",
    "\n",
    "        # size of feature vectors (= d)\n",
    "        self.feature_dim = sample_features.shape[0]\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # create weight vector with size of feature vectors, initialize with zeros\n",
    "        self.weights = np.zeros(self.feature_dim, dtype=np.float32)\n",
    "\n",
    "        # create eligibility vector (only when necessary)\n",
    "        if self.lmbda == 0:\n",
    "            # not needed for TD(0)\n",
    "            self.eligibility = None\n",
    "        else:\n",
    "            # initialize with zeros\n",
    "            self.eligibility = np.zeros(self.feature_dim, dtype=np.float32)\n",
    "\n",
    "    def evaluate(self, state):\n",
    "        w = self.weights  # weight vector\n",
    "\n",
    "        state_features = self.feature_fn(state)\n",
    "        v = np.dot(state_features, w)\n",
    "\n",
    "        return v\n",
    "\n",
    "    def update(self, state, target):\n",
    "        w = self.weights      # weight vector\n",
    "        e = self.eligibility  # eligibility vector\n",
    "        alpha = self.alpha    # learning rate\n",
    "        gamma = self.gamma    # discount factor\n",
    "        lmbda = self.lmbda    # value for TD(lambda)\n",
    "        #######################################################################\n",
    "        # TODO Update the weight vector using one of the update rules         #\n",
    "        # described above. The parameter `target` contains the TD target, so  #\n",
    "        # it does not need to be computed here, only the TD error. Update and #\n",
    "        # use the eligibility vector, only if lmbda != 0. Otherwise use the   #\n",
    "        # first update rule.                                                  #\n",
    "        #######################################################################\n",
    "\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        self.weights = w\n",
    "        self.eligibility = e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_linear_value_function():\n",
    "    yield 'evaluate()'\n",
    "\n",
    "    obs_space = gym.spaces.Discrete(10)\n",
    "    feature_fn = lambda s: np.array([float(s), float(s) * 2, float(s) * 3])\n",
    "\n",
    "    vf = LinearValueFunction(obs_space, alpha=0.1, gamma=0.9, lmbda=0.0, feature_fn=feature_fn)\n",
    "    vf.weights = np.array([-0.5, 0.4, 0.3])\n",
    "    for s, expected in zip([1, 2, 5, 10], [1.2, 2.4, 6.0, 12.0]):\n",
    "        v = vf.evaluate(s)\n",
    "        yield np.isclose(v, expected), f'The computed value is incorrect (error = {abs(v - expected):.5f})'\n",
    "        yield None\n",
    "\n",
    "    yield 'update() with lmbda == 0'\n",
    "\n",
    "    vf = LinearValueFunction(obs_space, alpha=0.1, gamma=0.9, lmbda=0.0, feature_fn=feature_fn)\n",
    "    for s, target, expected_w in [(1, 0.4, np.array([0.04, 0.08, 0.12])), (4, 0.65, np.array([-0.59599996, -1.1919999, -1.788]))]:\n",
    "        vf.update(s, target)\n",
    "        yield np.allclose(vf.weights, expected_w), f'The updated weights are incorrect (error = {np.sum(np.abs(vf.weights - expected_w)):.5f})'\n",
    "        yield None\n",
    "\n",
    "    yield None\n",
    "\n",
    "    yield 'update() with lmbda != 0'\n",
    "\n",
    "    vf = LinearValueFunction(obs_space, alpha=0.1, gamma=0.8, lmbda=0.6, feature_fn=feature_fn)\n",
    "    for s, target, expected_w in [(2, 0.3, np.array([0.06, 0.12, 0.18])), (5, -0.1, np.array([-2.5028, -5.0056, -7.5084]))]:\n",
    "        vf.update(s, target)\n",
    "        yield np.allclose(vf.weights, expected_w), f'The updated weights are incorrect (error = {np.sum(np.abs(vf.weights - expected_w)):.5f})'\n",
    "        yield None\n",
    "\n",
    "    yield None\n",
    "\n",
    "rl_tests.run_tests(test_linear_value_function())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We will evaluate the algorithms on the 1000-state random walk environment from the lecture.  \n",
    "To compare the estimated values to the true values, we load them from the file `random-walk-1000.npy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = np.load('random-walk-1000.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate TD(0) and TD($\\lambda$) using the three types of features.  \n",
    "We plot the estimated values for all states after 100, 500, and 5000 episodes.  \n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the environment\n",
    "env = rl_env.RandomWalk1000Env()\n",
    "\n",
    "gamma = 1.0   # discount factor\n",
    "lmbda = 0.9   # value for TD(lambda)\n",
    "alpha = 0.1   # learning rate\n",
    "\n",
    "# evaluate the value functions after these episodes\n",
    "milestones = [100, 500, 5000]\n",
    "num_episodes = milestones[-1]\n",
    "\n",
    "# create feature functions\n",
    "observation_space = env.observation_space\n",
    "one_hot_features = OneHotFeatures(observation_space)\n",
    "aggregation_features = StateAggregationFeatures(observation_space, group_size=100)\n",
    "normalization_features = StateNormalizationFeatures(observation_space)\n",
    "\n",
    "# create linear value functions for TD(0)\n",
    "one_hot_vf = LinearValueFunction(observation_space, alpha, gamma, 0, one_hot_features)\n",
    "aggregation_vf = LinearValueFunction(observation_space, alpha, gamma, 0, aggregation_features)\n",
    "normalization_vf = LinearValueFunction(observation_space, alpha, gamma, 0, normalization_features)\n",
    "\n",
    "# create linear value functions for TD(lambda)\n",
    "one_hot_lambda_vf = LinearValueFunction(observation_space, alpha, gamma, lmbda, one_hot_features)\n",
    "aggregation_lambda_vf = LinearValueFunction(observation_space, alpha, gamma, lmbda, aggregation_features)\n",
    "normalization_lambda_vf = LinearValueFunction(observation_space, alpha, gamma, lmbda, normalization_features)\n",
    "\n",
    "td_0_vfs = [one_hot_vf, aggregation_vf, normalization_vf]\n",
    "td_lambda_vfs = [one_hot_lambda_vf, aggregation_lambda_vf, normalization_lambda_vf]\n",
    "labels = ['One-hot encoding', 'State aggregation', 'State normalization']\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    while True:\n",
    "        action = 0  # Markov reward process, one action with no meaning\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # update all value functions\n",
    "        for vf in td_0_vfs + td_lambda_vfs:\n",
    "            # compute the TD target\n",
    "            if terminated:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + gamma * vf.evaluate(next_state)\n",
    "            # update the value function\n",
    "            vf.update(state, target)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    if (episode + 1) in milestones:\n",
    "        # compute and plot the values for all states\n",
    "\n",
    "        print(f'Episode {episode + 1}')\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 3), sharex=True, sharey=True)\n",
    "        axes[0].set_title('TD(0)')\n",
    "        axes[1].set_title('TD(lambda)')\n",
    "        axes[0].set_xlabel('State')\n",
    "        axes[1].set_xlabel('State')\n",
    "        axes[0].set_ylabel('Value')\n",
    "\n",
    "        num_states = env.observation_space.n\n",
    "        states = np.arange(1, num_states)\n",
    "        axes[0].plot(states, true_values, label='True value')\n",
    "        axes[1].plot(states, true_values, label='True value')\n",
    "\n",
    "        for vf, label in zip(td_0_vfs, labels):\n",
    "            values = np.zeros(num_states)\n",
    "            for state in range(num_states):\n",
    "                values[state] = vf.evaluate(state)\n",
    "            axes[0].plot(states, values[1:], label=label)\n",
    "        \n",
    "        for vf, label in zip(td_lambda_vfs, labels):\n",
    "            values = np.zeros(num_states)\n",
    "            for state in range(num_states):\n",
    "                values[state] = vf.evaluate(state)\n",
    "            axes[1].plot(states, values[1:], label=label)\n",
    "\n",
    "        axes[0].legend()\n",
    "        axes[1].legend()\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
