{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization (PPO)\n",
    "\n",
    "\n",
    "PPO is an actor-critic approach, i.e. it trains two neural networks:\n",
    "- *Actor*: The policy network that computes $\\pi_\\theta(a | s)$.\n",
    "- *Critic*: The value function network that computes $\\hat{v}(s, w)$.\n",
    "\n",
    "**Actor Optimization**:\n",
    "\n",
    "For a state and action $s_t,a_t$, define the probability ratio \n",
    "$$\n",
    "r_t({\\theta}) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)},\n",
    "$$\n",
    "where $\\pi_{\\theta_{\\text{old}}}$ is the policy that selected action $a_t$ in state $s_t$. PPO optimizes the actor $\\pi_\\theta$ through the clipped surrogate objective $\\mathcal{L}^\\text{clip}$, defined by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_1(\\theta) &= r_t({\\theta}) \\cdot a_\\theta(s_t, a_t) \\\\\n",
    "\\mathcal{L}_2(\\theta) &= \\text{clip}(r_t({\\theta}), 1 - \\epsilon, 1 + \\epsilon) \\cdot a_\\theta(s_t, a_t) \\\\\n",
    "\\mathcal{L}^\\text{clip}(\\theta) &= - \\mathbb{E}_{\\pi_{\\theta_{\\text{old}}}} \\left[\\text{min}(\\mathcal{L}_1(\\theta), \\mathcal{L}_2(\\theta)) \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "where the clip function will clip the value $r_t({\\theta})$ into the interval $[1-\\epsilon, 1+\\epsilon]$.\n",
    "\n",
    "**Generalized Advantage Estimates (GAE)**:\n",
    "\n",
    "The advantage $a_\\theta(s_t, a_t)$ is estimated through the so-called generalized advantage estimates. For a time-step $t$ and future time-step $T$, it is defined by\n",
    "$$\n",
    "\\begin{align}\n",
    "a_\\theta(s_t, a_t) &= \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + ... + (\\gamma \\lambda)^{T-t+1} \\delta_{T-1}, \\\\\n",
    "\\delta_t &= r_{t+1} + \\gamma \\hat{v}(s_{t+1}, w) - \\hat{v}(s_t, w)\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\gamma$ is the usual discount factor and $\\lambda$ is a hyperparamter. Try to answer the following questions:\n",
    "\n",
    "- What happens for $\\lambda = 0$. Does the advantage look familiar to you?\n",
    "- What happens for $\\lambda = 1$. Does the advantage look familiar to you?\n",
    "- Why is it called generalized advantage estimate?\n",
    "\n",
    "**Critic Optimization**:  \n",
    "\n",
    "In order to calculate the advantage, we optimize a critic network $\\hat{v}(s, w)$. This is done as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\text{target} &= a_\\theta(s_t, a_t) + \\hat{v}(s_t, w) \\\\\n",
    "\\mathcal{L}_\\text{VF}(w) &= \\mathbb{E}_{\\pi_\\theta} \\left[ (\\text{sg}(v_\\text{target}) - \\hat{v}(s_t, w))^2 \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "where sg denotes stop gradient.\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "In last week's exercise, we started $N$ environments, executed an environment step in each of them and performed the actor-critic update. PPO uses a generalized approach, where in each environment we execute $T$ many environment steps and then perform $K$ updates using the data. If $T=K=1$, we obtain the approach of last week's exercise. The PPO algorithm is explained below:\n",
    "\n",
    "- Create $N$ environment instances\n",
    "- for iteration = $1, 2, ...$\n",
    "    - for environment $1, 2, ..., N$\n",
    "        - execute $T$ steps in the environment using policy $\\pi_{\\theta_{\\text{old}}}$ for action selection\n",
    "        - save the generated data of states, actions, rewards, next states and terminations\n",
    "    - For epoch=1, ..., K\n",
    "        - Update the actor and critic network by minimizing the losses $\\mathcal{L}^\\text{clip}$ and $\\mathcal{L}_\\text{VF}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Make sure that the files `rl_gui.py` and `rl_tests.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import rl_gui\n",
    "import rl_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO Agent\n",
    "\n",
    "The class `PPO` is a subclass of `torch.nn.Module`, which takes care of the automatic differentiation.  \n",
    "It contains both networks for the actor and the critic.  \n",
    "Your task is to finish the implementation.\n",
    "\n",
    "We provide two utility functions that might be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_at_index(x, indices):\n",
    "    # Extracts the elements from x at the specified indices, i.e.,\n",
    "    # result[i] = x[i, indices[i]] for all i.\n",
    "    # - x has shape (batch_size, d)\n",
    "    # - indices has shape (batch_size)\n",
    "    # - the result has shape (batch_size)\n",
    "    return x.gather(-1, indices.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "def sample_categorical(probs):\n",
    "    # Samples from a categorical distribution (e.g. with probabilities computed with softmax).\n",
    "    # - probs has shape (batch_size, d)\n",
    "    # - the result has shape (batch_size)\n",
    "    return torch.multinomial(probs, 1, replacement=True).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, num_actions, gamma, lmbda, epsilon, num_steps):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.lmbda = lmbda\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "        # critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def select_actions(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # compute the scores for the given states\n",
    "        scores = self.actor(states)\n",
    "        # scores has shape (batch_size, num_actions)\n",
    "\n",
    "        probs = torch.softmax(scores, dim=1)\n",
    "        actions = sample_categorical(probs)\n",
    "        return actions\n",
    "\n",
    "    def compute_log_probs(self, states, actions):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # actions has shape (batch_size)\n",
    "        # compute the scores for the given states\n",
    "        scores = self.actor(states)\n",
    "        # scores has shape (batch_size, num_actions)\n",
    "        \n",
    "        logit_at_action = select_at_index(scores, actions)\n",
    "        log_sum_exp = torch.logsumexp(scores, dim=1)\n",
    "        log_probs = logit_at_action - log_sum_exp\n",
    "        \n",
    "        return log_probs\n",
    "\n",
    "    def compute_values(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # compute values for the batch of states using the critic network\n",
    "        values = self.critic(states).squeeze(-1)\n",
    "        # values has shape (batch_size)\n",
    "        return values\n",
    "\n",
    "    def compute_advantages(self, states, rewards, next_states, terminations):\n",
    "        \n",
    "        # for every of the N environments 0, ..., N-1, we collected T transitions\n",
    "        # state/next_state has shape (N * T, state_dim)\n",
    "        # rewards and terminations have shape (N * T)\n",
    "        # the states for environment i are saved in states[i*T:(i+1)*T, :]\n",
    "        # the rewards for environment i are saved in rewards[i*T:(i+1)*T]\n",
    "        # etc...\n",
    "\n",
    "        gamma = self.gamma  # discount factor\n",
    "        lmbda = self.lmbda  # for generalized advantage estimation\n",
    "        T = self.num_steps  # number of transitions per environment\n",
    "        num_states = states.size()[0]         # has size N * T\n",
    "        \n",
    "        ##############################################################################\n",
    "        # TODO Compute the advantages (GAE) and critic targets as described above.   #\n",
    "        # advantages and critic_targets should be torch.Tensors in the end.          #\n",
    "        # Hint: Iterate backwards for computing the advantages.                      #\n",
    "        # Remember to potentially reverse the advantage list afterwards.             #\n",
    "        # Don't forget to take care of the termination signals.                      #\n",
    "        # Be careful that some transitions can come from different environments.     #\n",
    "        ##############################################################################\n",
    "        \n",
    "\n",
    "\n",
    "        return advantages.detach(), critic_targets.detach()\n",
    "\n",
    "    def compute_loss(self, states, actions, advantages, critic_targets, log_probs_old):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # actions, advantages, critic_targets, and log_probs_old have shape (batch_size)\n",
    "        # log_probs_old are given by \\pi_\\theta_old(a_t | s_t)\n",
    "        \n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        #########################################################################\n",
    "        # TODO Compute the loss for the actor (L^clip) and the critic (L_VF) as #\n",
    "        # described above. The expectations are approximated by taking the      #\n",
    "        # average over the batch (i.e., use `x.mean()`).                        #\n",
    "        # Hint: you can use torch.clamp() for clipping values                   #\n",
    "        #########################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        # return both losses\n",
    "        return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "def test_ppo():\n",
    "    torch.manual_seed(42)\n",
    "    rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "    state_dim = 5\n",
    "    num_actions = 3\n",
    "    gamma = 0.8\n",
    "    lmbda = 0.95\n",
    "    epsilon = 0.2\n",
    "    num_collection_steps = 1\n",
    "    ac = PPO(state_dim, num_actions, gamma, lmbda, epsilon, num_collection_steps)\n",
    "    with torch.no_grad():\n",
    "        for layer in (ac.actor[0], ac.actor[2], ac.actor[4], ac.critic[0], ac.critic[2], ac.critic[4]):\n",
    "            mean = rng.uniform(-0.5, 0.5)\n",
    "            layer.weight[:] = torch.as_tensor(rng.normal(mean, 0.1, layer.weight.shape))\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    sample_states = lambda batch_size: torch.as_tensor(rng.standard_normal((batch_size, state_dim), dtype=np.float32))\n",
    "    sample_actions = lambda batch_size: torch.as_tensor(rng.choice(num_actions, batch_size))\n",
    "\n",
    "    yield 'compute_advantages()'\n",
    "    for expected_advantages in [\n",
    "        [0.01287864, 1.91237950, 0.50822473, 1.22912478],\n",
    "        [-0.18249962,  0.55023891,  0.32651904,  2.21070695,  0.97387546],\n",
    "        [0.73423117, -2.70612144, -1.03495073, -0.23954383, -0.12438969, 1.06986928, -0.15151261]\n",
    "    ]:\n",
    "        batch_size = len(expected_advantages)\n",
    "        states = sample_states(batch_size)\n",
    "        expected_advantages = torch.as_tensor(expected_advantages)\n",
    "        rewards = torch.randn(batch_size, dtype=torch.float32)\n",
    "        next_states = sample_states(batch_size)\n",
    "        terminations = torch.rand(batch_size) < 0.5\n",
    "        advantages, _ = ac.compute_advantages(states, rewards, next_states, terminations)\n",
    "\n",
    "        if (yield from rl_tests.check_torch_tensor(advantages, 'advantages', shape=(batch_size,), dtype=torch.float32)):\n",
    "            yield torch.allclose(advantages, torch.as_tensor(expected_advantages)), f'advantages are incorrect (error = {torch.sum(torch.abs(advantages - expected_advantages)).item()})'\n",
    "        yield None\n",
    "\n",
    "    yield 'compute_loss()'\n",
    "    for batch_size, expected_actor_loss, expected_critic_loss in [\n",
    "        (4, -0.27133098, 3.71168423),\n",
    "        (8, 0.30093294, 0.64200860),\n",
    "        (12, -0.15494117, 0.97636366)\n",
    "    ]:\n",
    "        states = sample_states(batch_size)\n",
    "        actions = sample_actions(batch_size)\n",
    "        next_states = sample_states(batch_size)\n",
    "        terminations = torch.rand(batch_size) < 0.5\n",
    "        rewards = torch.randn(batch_size, dtype=torch.float32)\n",
    "        #advantages = torch.randn(batch_size, dtype=torch.float32)\n",
    "        #critic_targets = torch.randn(batch_size, dtype=torch.float32)\n",
    "        advantages, critic_targets = ac.compute_advantages(states, rewards, next_states, terminations)\n",
    "        log_probs_old = torch.randn(batch_size, dtype=torch.float32)\n",
    "\n",
    "        actor_loss, critic_loss = ac.compute_loss(states, actions, advantages, critic_targets, log_probs_old)\n",
    "        if (yield from rl_tests.check_torch_tensor(actor_loss, 'actor_loss', shape=tuple(), dtype=torch.float32)):\n",
    "            expected_actor_loss = torch.tensor(expected_actor_loss)\n",
    "            yield torch.isclose(actor_loss, expected_actor_loss), f'actor_loss is incorrect (error = {(actor_loss - expected_actor_loss).item()})'\n",
    "            yield actor_loss.requires_grad, 'gradients disabled for actor_loss'\n",
    "        if (yield from rl_tests.check_torch_tensor(critic_loss, 'critic_loss', shape=tuple(), dtype=torch.float32)):\n",
    "            expected_critic_loss = torch.tensor(expected_critic_loss)\n",
    "            yield torch.isclose(critic_loss, expected_critic_loss), f'critic_loss is incorrect (error = {(actor_loss - expected_critic_loss).item()})'\n",
    "            yield critic_loss.requires_grad, 'gradients disabled for critic_loss'\n",
    "        yield None\n",
    "\n",
    "\n",
    "rl_tests.run_tests(test_ppo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We will evaluate the algorithm on the LunarLander environment.  \n",
    "Please read the documentation: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "If an error occurs, you might have to install the `box2d` environments first:  \n",
    "`pip install gymnasium[box2d]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(seed):\n",
    "    env_id = 'LunarLander-v3'\n",
    "    env = gym.make(env_id, render_mode='rgb_array')\n",
    "    env.reset(seed=seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env(seed=42)\n",
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'sum of rewards: {reward_sum:.2f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the policy using PPO.  \n",
    "We have provided some hyperparameters as a starting point, but the results could be improved.  \n",
    "Feel free to change the hyperparameters and try to find a better solution. The most interesting hyperparameters to change are\n",
    "\n",
    "- lmbda\n",
    "- epsilon\n",
    "- num_collection_steps\n",
    "- update_epochs\n",
    "\n",
    "For instance, setting lmbda=0 will give you the TD-target as used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "alpha = 1e-4       # learning rate\n",
    "gamma = 0.9        # discount factor\n",
    "lmbda = 0.95\n",
    "epsilon = 0.2\n",
    "num_collection_steps = 10\n",
    "update_epochs = 5\n",
    "\n",
    "num_envs = 16      # number of environment instances (= batch size)\n",
    "num_iterations = 20000  # number of iterations\n",
    "\n",
    "# create the environment instances, initialize with different random seeds\n",
    "envs = [create_env(seed=i) for i in range(num_envs)]\n",
    "\n",
    "# create the actor-critic agent and the optimizer\n",
    "state_dim = envs[0].observation_space.shape[0]\n",
    "num_actions = envs[0].action_space.n\n",
    "ac = PPO(state_dim, num_actions, gamma, lmbda, epsilon, num_collection_steps)\n",
    "optimizer = optim.Adam(ac.parameters(), lr=alpha)\n",
    "\n",
    "# reset all environment instances\n",
    "env_states = []\n",
    "for env in envs:\n",
    "    state, _ = env.reset()\n",
    "    env_states.append(torch.as_tensor(state))\n",
    "\n",
    "# only for plotting\n",
    "reward_sums = torch.zeros(num_envs)\n",
    "episode_rewards = []\n",
    "\n",
    "for _ in range(num_iterations // num_collection_steps):\n",
    "    ac.eval()  # switch to evaluation mode\n",
    "\n",
    "    # step all environment instances and store results in these lists:\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    terminations = []\n",
    "    truncations = []\n",
    "    \n",
    "    for i in range(num_envs):\n",
    "        \n",
    "        state = env_states[i]\n",
    "        for _ in range(num_collection_steps):\n",
    "        \n",
    "            action = ac.select_actions(state.unsqueeze(0)).squeeze()\n",
    "            next_state, reward, terminated, truncated, _ = envs[i].step(action.item())\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            terminations.append(terminated)\n",
    "            truncations.append(truncated)\n",
    "            \n",
    "            reward_sums[i] += reward\n",
    "            \n",
    "            if terminated:\n",
    "                state, _ = envs[i].reset()\n",
    "                state = torch.as_tensor(state)\n",
    "                \n",
    "                episode_rewards.append(reward_sums[i].item())\n",
    "                reward_sums[i] = 0\n",
    "            else:\n",
    "                state = torch.as_tensor(next_state)\n",
    "        \n",
    "        env_states[i] = deepcopy(state)\n",
    "            \n",
    "\n",
    "    # convert the lists to tensors\n",
    "    states = torch.cat([state.unsqueeze(0) for state in states])\n",
    "    actions = torch.as_tensor(np.array(actions, dtype=np.int64))\n",
    "    rewards = torch.as_tensor(np.array(rewards, dtype=np.float32))\n",
    "    next_states = torch.as_tensor(np.array(next_states))\n",
    "    terminations = torch.as_tensor(np.array(terminations))\n",
    "    truncations = torch.as_tensor(np.array(truncations))\n",
    "\n",
    "    # optimize the actor and critic networks\n",
    "    ac.train()  # switch to training mode\n",
    "    for _ in range(update_epochs):\n",
    "    \n",
    "        advantages, critic_targets = ac.compute_advantages(states, rewards, next_states, terminations)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            log_probs_old = ac.compute_log_probs(states, actions)\n",
    "        actor_loss, critic_loss = ac.compute_loss(states, actions, advantages, critic_targets, log_probs_old)\n",
    "        \n",
    "        total_loss = actor_loss + critic_loss\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "plt.title('LunarLander')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Sum of rewards')\n",
    "plt.plot(episode_rewards, c='C0', alpha=0.3)\n",
    "plt.plot(np.convolve(episode_rewards, np.ones(20) / 20, mode='valid'), c='C0');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the learned policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "state, _ = env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "\n",
    "ac.eval()\n",
    "\n",
    "while True:\n",
    "    action = ac.select_actions(torch.as_tensor(state).unsqueeze(0)).item()\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'action: {action}, sum of rewards: {reward_sum:.2f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
