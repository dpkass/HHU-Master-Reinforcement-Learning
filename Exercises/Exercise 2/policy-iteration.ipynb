{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration\n",
    "\n",
    "Policy iteration is an algorithm to find an optimal policy for an MDP. Written in pseudocode it looks like this:  \n",
    "\n",
    "Repeat two steps until the policy converges:\n",
    "- **Policy evaluation**, i.e. compute the value function $v_\\pi$ of the policy $\\pi$:\n",
    "   1. Initialize value function $v_0(s)$ for all states $s \\in \\mathcal{S}$ arbitrarily.\n",
    "   2. Repeat until the value function converges:\n",
    "      $$\\begin{aligned}\n",
    "      & v_{k+1}(s) = \\sum_a \\pi(a|s) \\left( \\mathcal{R}(s,a) + \\gamma \\sum_{s'} \\mathcal{P}(s'|s,a) v_k(s') \\right)\\\\\n",
    "      & \\text{for all } s \\in \\mathcal{S}\n",
    "      \\end{aligned}$$\n",
    "      The value function is converged if $$|v_{k+1}(s) - v_k(s)| < \\epsilon$$ for all $s \\in \\mathcal{S}$.\n",
    "- **Policy improvement**, i.e. find a better policy $\\pi'$ using the value function:\n",
    "  $$\\begin{aligned}\n",
    "  & \\pi'(s) = \\arg\\max_a \\left( \\mathcal{R}(s,a) + \\gamma \\sum_{s'} \\mathcal{P}(s'|s,a) v_\\pi(s') \\right) \\\\\n",
    "  & \\text{for all } s \\in \\mathcal{S}\n",
    "  \\end{aligned}$$\n",
    "  The policy is converged if $\\pi'(s) = \\pi(s)$ for all $s \\in \\mathcal{S}$.\n",
    "- (Set $\\pi = \\pi'$)\n",
    "\n",
    "Both update rules above make use of the same expression on the right-hand side, which is known as the Q-function or action-value function:\n",
    "$$q_\\pi(s,a) = \\mathcal{R}(s,a) + \\gamma \\sum_{s'} \\mathcal{P}(s'|s,a) v_\\pi(s')$$\n",
    "We will talk more about it in the next lecture. We can use it now to simplify the update rules:\n",
    "$$\\begin{aligned}\n",
    "v_{k+1}(s) & = \\sum_a \\pi(a|s) \\, q_k(s,a) \\\\\n",
    "\\pi'(s) & = \\arg\\max_a q_\\pi(s,a)\n",
    "\\end{aligned}$$\n",
    "Note that we need to convert the deterministic policy into a stochastic one to be able to use it for policy evalution:\n",
    "$$\\pi(a|s) = \\begin{cases}\n",
    "  1 & \\text{ if } a = \\pi(s) \\\\\n",
    "  0 & \\text{ otherwise}\n",
    "\\end{cases}$$\n",
    "<details>\n",
    "<summary>Minor note (click)</summary>\n",
    "Alternatively, we could make use of the fact that the policy is deterministic and change the update rule of policy evaluation to:\n",
    "\n",
    "$$v_{k+1}(s) = q_k(s,\\pi(s))$$\n",
    "\n",
    "However, in this exercise we use the more general stochastic version above.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Make sure that the files `rl_agent.py`, `rl_env.py`, `rl_gui.py` and `rl_tests.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import rl_agent\n",
    "import rl_env\n",
    "import rl_gui\n",
    "import rl_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement policy iteration in the class `PolicyIterationAgent` below.  \n",
    "Follow the instructions in the methods `compute_q()`, `update_v()` and `policy_improvement()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationAgent(rl_agent.TabularAgent):\n",
    "\n",
    "    def __init__(self, env, gamma):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Resets the agent\n",
    "        # Initialize the stochastic policy with uniform probabilities\n",
    "        self.pi = np.full((self.num_states, self.num_actions), 1 / self.num_actions)\n",
    "        # The policy did not converge yet\n",
    "        self.pi_converged = False\n",
    "        # Initialize the value function as an array of zeros\n",
    "        self.v = np.zeros(self.num_states, dtype=float)\n",
    "        # The value function did not converge yet\n",
    "        self.v_converged = False\n",
    "\n",
    "    def policy(self, state):\n",
    "        # Sample an action from the stochastic policy\n",
    "        action = np.random.choice(self.num_actions, p=self.pi[state])\n",
    "        return action\n",
    "    \n",
    "    def value(self, state):\n",
    "        # Lookup in the value function array\n",
    "        return self.v[state]\n",
    "\n",
    "    def compute_q(self):\n",
    "        gamma = self.gamma\n",
    "        v = self.v\n",
    "        # Get the transition probabilities and reward function\n",
    "        # from the environment\n",
    "        P = self.env.P\n",
    "        R = self.env.R\n",
    "        #######################################################################\n",
    "        # TODO: This methods compute the q-function using the value function  #\n",
    "        # with the formula:                                                   #\n",
    "        #     q(s,a) = R(s,a) + gamma * sum_s' P(s'|s,a) v(s')                #\n",
    "        # Note that the next states are stored in the last dimension of P,    #\n",
    "        # i.e. the probability of s' given s, a is stored in P[s, a, s'].     #\n",
    "        # You can implement this using for-loops. Alternatively, a vectorized #\n",
    "        # implementation can be written in a single line of code.             #\n",
    "        #######################################################################\n",
    "        \n",
    "    \n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        return q\n",
    "\n",
    "    def update_v(self, epsilon=1e-4):\n",
    "        # Store the old values to check for convergence\n",
    "        old_v = np.copy(self.v)\n",
    "        # Compute the action value function\n",
    "        q = self.compute_q()\n",
    "        pi = self.pi\n",
    "        #######################################################################\n",
    "        # TODO: This method applies one step of policy evaluation and checks  #\n",
    "        # if the value function converged. We already implemented the         #\n",
    "        # convergence check below. Remember to store the result in self.v     #\n",
    "        # You can implement this using for-loops. Alternatively, a vectorized #\n",
    "        # implementation can be written in a single line of code.             #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "\n",
    "        # Check if the value function converged\n",
    "        self.v_converged = np.allclose(self.v, old_v, atol=epsilon)\n",
    "\n",
    "    def policy_evaluation(self, epsilon=1e-4):\n",
    "        # Run update_v() until the value function converges\n",
    "        while not self.v_converged:\n",
    "            self.update_v(epsilon)\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        # Improve the policy by taking the maximum over the q-function\n",
    "\n",
    "        old_pi = np.copy(self.pi)\n",
    "        q = self.compute_q()\n",
    "        #######################################################################\n",
    "        # TODO: This method improves the policy by taking the maximum over    #\n",
    "        # the q-function. We already implemented the conversion of the        #\n",
    "        # deterministic policy into a stochastic one. Your task is to create  #\n",
    "        # a NumPy array of type integer called \"max_actions\" that contains    #\n",
    "        # the best action for each state.                                     #\n",
    "        # You can implement this using for-loops. Alternatively, a vectorized #\n",
    "        # implementation can be written in a single line of code.             #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "\n",
    "        # Convert to stochastic policy with one-hot probabilities\n",
    "        self.pi = np.eye(self.num_actions)[max_actions]\n",
    "        # Check if the policy converged\n",
    "        self.pi_converged = np.all(self.pi == old_pi)\n",
    "\n",
    "        if not self.pi_converged:\n",
    "            # The value function did not converge, since we have a new policy\n",
    "            self.v_converged = False\n",
    "\n",
    "    def policy_iteration(self, epsilon=1e-4):\n",
    "        # Repeat policy evaluation and improvement until the policy converges\n",
    "        while not self.pi_converged:\n",
    "            self.policy_evaluation(epsilon)\n",
    "            self.policy_improvement()\n",
    "\n",
    "    # This method is used for the GUI\n",
    "    # You don't have to understand the code\n",
    "    def interactive_optimization(self):\n",
    "        from rl_gui import RLCmd, RLParamsResult\n",
    "\n",
    "        def update_params(params):\n",
    "            if params['gamma'] != self.gamma:\n",
    "                self.gamma = params['gamma']\n",
    "                if self.v_converged or self.pi_converged:\n",
    "                    self.v_converged = False\n",
    "                    self.pi_converged = False\n",
    "                    # Get out of the 'Policy evaluation converged'/'Policy is optimal' state\n",
    "                    return RLParamsResult.RESET_GENERATOR\n",
    "\n",
    "        yield RLCmd.Init(options={'eval_step': 'Evaluate 1 step',\n",
    "                                  'eval': 'Evaluate policy',\n",
    "                                  'improve': 'Improve policy',\n",
    "                                  'complete': 'Finish optimization',\n",
    "                                  'reset': 'Reset agent'},\n",
    "                         params={'gamma': ('Discount factor', 'float', self.gamma, 0.0, 1.0 - 1e-4)},\n",
    "                         params_callback=update_params)\n",
    "\n",
    "        while not self.pi_converged:\n",
    "            option = None\n",
    "            while not self.v_converged and not self.pi_converged:\n",
    "                if option is None or option == 'eval_step':\n",
    "                    option = yield RLCmd.WaitForOption(active=['eval', 'eval_step', 'complete', 'reset'],\n",
    "                                                       step='eval_step', message='Policy evaluation not converged')\n",
    "\n",
    "                if option == 'complete':\n",
    "                    self.policy_iteration()\n",
    "                elif option == 'reset':\n",
    "                    self.reset()\n",
    "                    option = None\n",
    "                else:\n",
    "                    self.update_v()\n",
    "\n",
    "            if self.pi_converged:\n",
    "                break\n",
    "\n",
    "            option = yield RLCmd.WaitForOption(active=['complete', 'improve', 'reset'],\n",
    "                                               step='improve', message='Policy evaluation converged')\n",
    "            if option == 'complete':\n",
    "                self.policy_iteration()\n",
    "            elif option == 'reset':\n",
    "                self.reset()\n",
    "            else:\n",
    "                self.policy_improvement()\n",
    "\n",
    "        option = yield RLCmd.WaitForOption(active=['reset'], message='Policy is optimal')\n",
    "        assert option == 'reset', option\n",
    "        self.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pi_agent():\n",
    "    env = rl_env.default_5x5_maze(model_based=True)\n",
    "    rng = None\n",
    "\n",
    "    def seed():\n",
    "        nonlocal rng\n",
    "        rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "\n",
    "    def create_agent(gamma):\n",
    "        agent = PolicyIterationAgent(env, gamma)\n",
    "        pi = rng.uniform(0, 1, (agent.num_states, agent.num_actions))\n",
    "        pi /= np.sum(pi, axis=1, keepdims=True)\n",
    "        agent.pi = pi\n",
    "        agent.v = rng.standard_normal(agent.num_states)\n",
    "        return agent\n",
    "\n",
    "    yield 'compute_q()'\n",
    "    seed()\n",
    "    for expected_sum in [-18.462667, -4.839807, 14.589116]:\n",
    "        agent = create_agent(gamma=0.8)\n",
    "        q = agent.compute_q()\n",
    "        if (yield from rl_tests.check_numpy_array(q, name='The result of compute_q()', shape=(agent.num_states, agent.num_actions), dtype=np.floating)):\n",
    "            q_sum = np.sum(q)\n",
    "            yield np.isclose(q_sum, expected_sum, atol=1e-5), f'The computed q-values are incorrect (error = {abs(expected_sum - q_sum):.5f})'\n",
    "        yield None\n",
    "    \n",
    "    yield 'update_v()'\n",
    "    seed()\n",
    "    for expected_sum in [-4.457114, -1.007184, 4.208458]:\n",
    "        agent = create_agent(gamma=0.8)\n",
    "        agent.update_v()\n",
    "        if (yield from rl_tests.check_numpy_array(agent.v, name='self.v', shape=(agent.num_states,), dtype=np.floating)):\n",
    "            v_sum = np.sum(agent.v)\n",
    "            yield np.isclose(v_sum, expected_sum, atol=1e-5), f'The updated values are incorrect (error = {abs(expected_sum - v_sum):.5f})'\n",
    "        yield None\n",
    "    \n",
    "    yield 'policy_improvement()'\n",
    "    seed()\n",
    "    for expected_indices in [\n",
    "        np.array([3, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 3, 1, 3, 1, 0, 0, 0, 0, 2, 2, 0]),\n",
    "        np.array([0, 0, 3, 0, 2, 1, 1, 2, 0, 0, 0, 2, 3, 2, 3, 1, 3, 1, 3, 0, 1, 0]),\n",
    "        np.array([3, 1, 0, 0, 3, 0, 2, 3, 3, 0, 3, 0, 0, 1, 1, 2, 2, 2, 2, 1, 3, 0])\n",
    "    ]:\n",
    "        agent = create_agent(gamma=0.8)\n",
    "        agent.policy_improvement()\n",
    "        if (yield from rl_tests.check_numpy_array(agent.pi, name='self.pi', shape=(agent.num_states, agent.num_actions), dtype=np.floating)):\n",
    "            indices = np.argmax(agent.pi, axis=1)\n",
    "            yield np.all(indices == expected_indices), f'The improved policy is incorrect'\n",
    "        yield None\n",
    "\n",
    "rl_tests.run_tests(test_pi_agent())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all of your tests passed, you can see your agent in action in the following code cell.\n",
    "\n",
    "Sometimes there is a strange bug and the environment is rendered multiple times. In that case you may have to restart the notebook and reopen the browser tab or restart the editor (e.g. in VS Code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a GUI for the maze environment\n",
    "env = rl_env.default_5x5_maze(model_based=True)\n",
    "# you can try the bigger maze by uncommenting the next line\n",
    "#env = rl_env.default_8x8_maze(model_based=True)\n",
    "\n",
    "gamma = 0.9  # discount factor\n",
    "agents = {'Random': rl_agent.RandomAgent(env),\n",
    "          'Policy Iteration': PolicyIterationAgent(env, gamma)}\n",
    "\n",
    "rl_gui.RLGui(env, agents=agents, max_steps=1000, render_mode='rgb_array')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
