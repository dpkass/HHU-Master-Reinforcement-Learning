{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration\n",
    "\n",
    "Value iteration is an algorithm to find an optimal policy for an MDP. Written in pseudocode it looks like this:  \n",
    "\n",
    "1. Initialize value function $v_0(s)$ for all states $s \\in \\mathcal{S}$ arbitrarily.\n",
    "2. Repeat until the value function converges:\n",
    "      $$\\begin{aligned}\n",
    "      & v_{k+1}(s) = \\max_a \\left( \\mathcal{R}(s,a) + \\gamma \\sum_{s'} \\mathcal{P}(s'|s,a) v_k(s') \\right)\\\\\n",
    "      & \\text{for all } s \\in \\mathcal{S}\n",
    "      \\end{aligned}$$\n",
    "      The value function is converged if $$|v_{k+1}(s) - v_k(s)| < \\epsilon$$ for all $s \\in \\mathcal{S}$.\n",
    "3. Derive the optimal policy from the last value function $v_K$:\n",
    "      $$\\begin{aligned}\n",
    "      & \\pi_*(s) = \\arg\\max_a \\left( \\mathcal{R}(s,a) + \\gamma \\sum_{s'} \\mathcal{P}(s'|s,a) v_K(s') \\right) \\\\\n",
    "      & \\text{for all } s \\in \\mathcal{S}\n",
    "      \\end{aligned}$$\n",
    "\n",
    "Both update rules above make use of the action value function on the right-hand side.  \n",
    "We can simplify the update rules:\n",
    "$$\\begin{aligned}\n",
    "v_{k+1}(s) & = \\max_a q_k(s,a) \\\\\n",
    "\\pi_*(s) & = \\arg\\max_a q_K(s,a)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Make sure that the files `rl_agent.py`, `rl_env.py`, `rl_gui.py` and `rl_tests.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import rl_agent\n",
    "import rl_env\n",
    "import rl_gui\n",
    "import rl_tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement value iteration in the class `ValueIterationAgent` below.  \n",
    "Follow the instructions in the method `update_v()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationAgent(rl_agent.TabularAgent):\n",
    "\n",
    "    def __init__(self, env, gamma, tolerance=1e-4):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma\n",
    "        self.tolerance = tolerance\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Resets the agent\n",
    "        self.v = np.zeros(self.num_states, dtype=float)\n",
    "        # The value function did not converge yet\n",
    "        self.converged = False\n",
    "\n",
    "        # Initialize policy with uniform probabilities (not necessary)\n",
    "        self.pi = np.full((self.num_states, self.num_actions), 1 / self.num_actions)\n",
    "        # In this implementation we set pi_stale to True when the value function\n",
    "        # was changed. This allows us to safe computations by only computing\n",
    "        # pi when it is necessary.\n",
    "        self.pi_outdated = False\n",
    "\n",
    "    def policy(self, state):\n",
    "        # Derive the policy from the value function, if necessary\n",
    "        self.update_pi()\n",
    "        # Sample an action from the stochastic policy\n",
    "        action = np.random.choice(self.num_actions, p=self.pi[state])\n",
    "        return action\n",
    "    \n",
    "    def value(self, state):\n",
    "        # Lookup in the value function array\n",
    "        return self.v[state]\n",
    "\n",
    "    def compute_q(self):\n",
    "        # Derive Q from V using the environment's dynamics,\n",
    "        # i.e., apply the mutually recursive Bellman expectation equation:\n",
    "        # q(s,a) = R(s,a) + gamma * sum_s' P(s'|s,a) v(s')\n",
    "        gamma = self.gamma\n",
    "        v = self.v\n",
    "        # Get the transition probabilities and reward function\n",
    "        # from the environment\n",
    "        P = self.env.P\n",
    "        R = self.env.R\n",
    "        q = R + gamma * np.sum(P * v, axis=2)\n",
    "        return q\n",
    "\n",
    "    def update_v(self):\n",
    "        # Store the old values to check for convergence\n",
    "        old_v = np.copy(self.v)\n",
    "        # Compute the action value function\n",
    "        q = self.compute_q()\n",
    "        #######################################################################\n",
    "        # TODO: This method applies one step of value iteration and checks    #\n",
    "        # if the value function converged. We already implemented the         #\n",
    "        # convergence check below. Remember to store the result in self.v     #\n",
    "        # You can implement this using for-loops. Alternatively, a vectorized #\n",
    "        # implementation can be written in a single line of code.             #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "\n",
    "        # Check if the value function converged\n",
    "        self.converged = np.allclose(self.v, old_v, atol=self.tolerance)\n",
    "        # Mark the policy as outdated\n",
    "        self.pi_outdated = True\n",
    "\n",
    "    def update_pi(self):\n",
    "        # Derive the policy from the value function, if necessary\n",
    "        if self.pi_outdated:\n",
    "            # Compute the action value function\n",
    "            q = self.compute_q()\n",
    "            # Take the argmax over the action values\n",
    "            indices = np.argmax(q, axis=1)\n",
    "            # Convert to stochastic policy with one-hot probabilities\n",
    "            self.pi = np.eye(self.num_actions)[indices]\n",
    "            # Now the policy corresponds to the value function\n",
    "            self.pi_outdated = False\n",
    "\n",
    "    def value_iteration(self):\n",
    "        # Run update_v() until the value function converges\n",
    "        while not self.converged:\n",
    "            self.update_v()\n",
    "        self.update_pi()\n",
    "\n",
    "    # This method is used for the GUI\n",
    "    # You don't have to understand the code\n",
    "    def interactive_optimization(self):\n",
    "        from rl_gui import RLCmd, RLParamsResult\n",
    "\n",
    "        def update_params(params):\n",
    "            gamma = params['gamma']\n",
    "            if gamma != self.gamma:\n",
    "                self.gamma = gamma\n",
    "                if self.converged:\n",
    "                    self.converged = False\n",
    "                    # Get out of the 'Policy is optimal' state\n",
    "                    return RLParamsResult.RESET_GENERATOR\n",
    "\n",
    "        yield RLCmd.Init(options={'step': 'Value iteration',\n",
    "                                  'complete': 'Finish optimization',\n",
    "                                  'reset': 'Reset agent'},\n",
    "                         params={'gamma': ('Discount factor', 'float', self.gamma, 0.0, 1.0 - 1e-4)},\n",
    "                         params_callback=update_params)\n",
    "\n",
    "        option = None\n",
    "        while not self.converged:\n",
    "            if option is None or option == 'step':\n",
    "                option = yield RLCmd.WaitForOption(active=['step', 'complete', 'reset'],\n",
    "                                                   step='step', interval=200)\n",
    "\n",
    "            if option == 'reset':\n",
    "                self.reset()\n",
    "                option = None\n",
    "            else:\n",
    "                self.update_v()\n",
    "\n",
    "        option = yield RLCmd.WaitForOption(active=['reset'], message='Policy is optimal')\n",
    "        assert option == 'reset', option\n",
    "        self.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vi_agent():\n",
    "    env = rl_env.default_5x5_maze(model_based=True)\n",
    "    rng = None\n",
    "\n",
    "    def seed():\n",
    "        nonlocal rng\n",
    "        rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "\n",
    "    def create_agent(gamma):\n",
    "        agent = ValueIterationAgent(env, gamma)\n",
    "        pi = rng.uniform(0, 1, (agent.num_states, agent.num_actions))\n",
    "        pi /= np.sum(pi, axis=1, keepdims=True)\n",
    "        agent.pi = pi\n",
    "        agent.v = rng.standard_normal(agent.num_states)\n",
    "        return agent\n",
    "\n",
    "    yield 'update_v()'\n",
    "    seed()\n",
    "    for expected_sum in [9.016962, 16.691778, 20.439101]:\n",
    "        agent = create_agent(gamma=0.8)\n",
    "        agent.update_v()\n",
    "        if (yield from rl_tests.check_numpy_array(agent.v, name='self.v', shape=(agent.num_states,), dtype=np.floating)):\n",
    "            v_sum = np.sum(agent.v)\n",
    "            yield np.isclose(v_sum, expected_sum, atol=1e-5), f'The updated values are incorrect (error = {abs(expected_sum - v_sum):.5f})'\n",
    "        yield None\n",
    "\n",
    "rl_tests.run_tests(test_vi_agent())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all of your tests passed, you can see your agent in action in the following code cell.\n",
    "\n",
    "Sometimes there is a strange bug and the environment is rendered multiple times. In that case you may have to restart the notebook and reopen the browser tab or restart the editor (e.g. in VS Code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a GUI for the maze environment\n",
    "env = rl_env.default_5x5_maze(model_based=True)\n",
    "# you can try the bigger maze by uncommenting the next line\n",
    "#env = rl_env.default_8x8_maze(model_based=True)\n",
    "\n",
    "gamma = 0.9  # discount factor\n",
    "agents = {'Random': rl_agent.RandomAgent(env),\n",
    "          'Value Iteration': ValueIterationAgent(env, gamma)}\n",
    "\n",
    "rl_gui.RLGui(env, agents=agents, max_steps=1000, render_mode='rgb_array')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
