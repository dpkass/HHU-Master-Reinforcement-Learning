{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-learning\n",
    "\n",
    "In this notebook we will implement double Q-learning, which uses two Q-functions $Q_1$ and $Q_2$, one for action selection, the other for the target value. During training, the roles of of $Q_1$ and $Q_2$ are alternated.\n",
    "\n",
    "The update rule for $Q_1$ is:\n",
    "$$\n",
    "Q_1(s_t,a_t) \\leftarrow Q_1(s_t,a_t) + \\alpha (r_{t+1} + \\gamma Q_2(s_{t+1}, \\arg\\max_{a'} Q_1(s_{t+1},a')) - Q_1(s_1, a_t))\n",
    "$$\n",
    "The update rule for $Q_2$ is:\n",
    "$$\n",
    "Q_2(s_t,a_t) \\leftarrow Q_2(s_t,a_t) + \\alpha (r_{t+1} + \\gamma Q_1(s_{t+1}, \\arg\\max_{a'} Q_2(s_{t+1},a')) - Q_2(s_1, a_t))\n",
    "$$\n",
    "\n",
    "For the implementation, there are two missing details:\n",
    "- When do we update $Q_1$ and when $Q_2$?\n",
    "- Which action values do we use for $\\epsilon$-greedy action selection?\n",
    "\n",
    "In this notebook, we will flip a coin to update either $Q_1$ or $Q_2$ with $50\\%$ probability.  \n",
    "The action values that are used for action selection are the average of $Q_1$ and $Q_2$.  \n",
    "To implement this, we will store a third Q-function $Q$, which gets updated every time when we update $Q_1$ or $Q_2$ with the following update rule:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow (Q_1(s_t,a_t) + Q_2(s_t,a_t)) / 2\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Make sure that the files `rl_agent.py`, `rl_env.py` and `rl_tests.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rl_agent\n",
    "import rl_env\n",
    "import rl_tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement double Q-learning as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent(rl_agent.TabularAgent):\n",
    "\n",
    "    def __init__(self, env, gamma, epsilon, alpha, rng=None):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.epsilon = epsilon  # epsilon-greedy probability\n",
    "        self.alpha = alpha      # learning rate\n",
    "        self.rng = rng if rng is not None else np.random.default_rng()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create arrays for action values\n",
    "        self.q1 = np.zeros((self.num_states, self.num_actions), dtype=float)\n",
    "        self.q2 = np.zeros((self.num_states, self.num_actions), dtype=float)\n",
    "        self.q = np.zeros((self.num_states, self.num_actions), dtype=float)\n",
    "        # Create array for policy distribution (initialized uniformly)\n",
    "        self.pi = np.full((self.num_states, self.num_actions), 1 / self.num_actions)\n",
    "\n",
    "    def policy(self, state):\n",
    "        # Sample an action according to pi\n",
    "        action_probs = self.pi[state]\n",
    "        action = self.rng.choice(self.num_actions, p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def value(self, state):\n",
    "        # Compute the state value from q and pi\n",
    "        return np.sum(self.pi[state] * self.q[state])\n",
    "\n",
    "    def action_value(self, state, action):\n",
    "        # Lookup the action value in q\n",
    "        return self.q[state, action]\n",
    "\n",
    "    def policy_evaluation(self, state, action, reward, next_state, terminated, truncated):\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "        q = self.q\n",
    "        q1 = self.q1\n",
    "        q2 = self.q2\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO Perform one step of policy evaluation. Use `self.rng.random()` #\n",
    "        # to decide which Q-function is updated (it returns a random value    #\n",
    "        # between 0 and 1). Also update `q` as described above. Remember to   #\n",
    "        # check whether the episode is terminated.                            #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        # Update the policy using epsilon-greedy policy improvement\n",
    "        q = self.q\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        # Find the maximizing actions\n",
    "        max_q = np.max(q, axis=1, keepdims=True)\n",
    "        max_mask = q == max_q\n",
    "        num_max = np.sum(max_mask, axis=1, keepdims=True)\n",
    "\n",
    "        # Compute the probabilities\n",
    "        num_actions = q.shape[1]\n",
    "        pi = ((1 - epsilon) / num_max) * max_mask\n",
    "        pi += epsilon / num_actions\n",
    "        self.pi = pi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_double_q_learning_agent():\n",
    "    env = rl_env.MaximizationBiasEnv(num_actions=10)\n",
    "    rng = None\n",
    "\n",
    "    def seed(offset=0):\n",
    "        nonlocal rng\n",
    "        rng = np.random.Generator(np.random.PCG64(seed=42 + offset))\n",
    "        env.reset(seed=42 + offset)\n",
    "\n",
    "    def create_agent(gamma, epsilon, alpha):\n",
    "        return DoubleQLearningAgent(env, gamma, epsilon, alpha, rng=rng)\n",
    "\n",
    "    yield 'policy_evaluation()'\n",
    "    seed()\n",
    "    state, _ = env.reset()\n",
    "    for expected_sum in [0.8329420394580342, -0.5262261767947287, 0.7997062230980709]:\n",
    "        agent = create_agent(gamma=0.8, epsilon=0.01, alpha=0.1)\n",
    "        for _ in range(10):\n",
    "            s, next_s = [rng.integers(0, agent.num_states) for _ in range(2)]\n",
    "            a = rng.integers(0, agent.num_actions)\n",
    "            r = rng.uniform(-5.0, 5.0)\n",
    "            agent.policy_evaluation(s, a, r, next_s, False, False)\n",
    "\n",
    "        if (yield from rl_tests.check_numpy_array(agent.q, name='self.q', shape=(agent.num_states, agent.num_actions), dtype=np.floating)):\n",
    "            q_sum = np.sum(agent.q)\n",
    "            yield np.isclose(q_sum, expected_sum, atol=1e-5), f'The updated action values are incorrect (error = {abs(expected_sum - q_sum):.5f})'\n",
    "        yield None\n",
    "\n",
    "rl_tests.run_tests(test_double_q_learning_agent())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Now we evaluate double Q-learning and Q-learning on the simple environment from Figure 6.5 in the textbook http://incompleteideas.net/book/the-book-2nd.html  \n",
    "which we have already seen in the lecture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already implemented everything below.\n",
    "\n",
    "First, the Q-learning agent for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(rl_agent.TabularAgent):\n",
    "\n",
    "    def __init__(self, env, gamma, epsilon, alpha, rng=None):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.epsilon = epsilon  # epsilon-greedy probability\n",
    "        self.alpha = alpha      # learning rate\n",
    "        self.rng = rng if rng is not None else np.random.default_rng()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create array for action values\n",
    "        self.q = np.zeros((self.num_states, self.num_actions), dtype=float)\n",
    "        # Create array for policy distribution (initialized uniformly)\n",
    "        self.pi = np.full((self.num_states, self.num_actions), 1 / self.num_actions)\n",
    "\n",
    "    def policy(self, state):\n",
    "        # Sample an action according to pi\n",
    "        action_probs = self.pi[state]\n",
    "        action = self.rng.choice(self.num_actions, p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def value(self, state):\n",
    "        # Compute the state value from q and pi\n",
    "        return np.sum(self.pi[state] * self.q[state])\n",
    "\n",
    "    def action_value(self, state, action):\n",
    "        # Lookup the action value in q\n",
    "        return self.q[state, action]\n",
    "\n",
    "    def policy_evaluation(self, state, action, reward, next_state, terminated, truncated):\n",
    "        # Update the action value with the Q-learning update\n",
    "\n",
    "        if terminated:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.q[next_state])\n",
    "\n",
    "        q = self.q\n",
    "        q[state, action] += self.alpha * (target - q[state, action])\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        # Update the policy using epsilon-greedy policy improvement\n",
    "        q = self.q\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        # Find the maximizing actions\n",
    "        max_q = np.max(q, axis=1, keepdims=True)\n",
    "        max_mask = q == max_q\n",
    "        num_max = np.sum(max_mask, axis=1, keepdims=True)\n",
    "\n",
    "        # Compute the probabilities\n",
    "        num_actions = q.shape[1]\n",
    "        pi = ((1 - epsilon) / num_max) * max_mask\n",
    "        pi += epsilon / num_actions\n",
    "        self.pi = pi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two code cells evaluate both agents on the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, num_episodes):\n",
    "    # Train the agent and return the first action from each episode.\n",
    "\n",
    "    first_actions = []\n",
    "\n",
    "    # In state 2 (A), there are only two valid actions, so we manually\n",
    "    # update `pi` to only choose `left` or `right`.\n",
    "\n",
    "    def correct_pi():\n",
    "        agent.pi[2, 2:] = 0.0\n",
    "        agent.pi[2] /= agent.pi[2, :2].sum()\n",
    "\n",
    "    correct_pi()\n",
    "\n",
    "    # The usual training loop\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        action = agent.policy(state)\n",
    "        first_actions.append(action)\n",
    "\n",
    "        while True:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            agent.policy_evaluation(state, action, reward, next_state, terminated, truncated)\n",
    "            agent.policy_improvement()\n",
    "            correct_pi()\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = agent.policy(state)\n",
    "\n",
    "    return first_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rl_env.MaximizationBiasEnv(num_actions=10)\n",
    "gamma = 1.0    # discount factor\n",
    "epsilon = 0.1  # epsilon-greedy probability\n",
    "alpha = 0.1    # learning rate\n",
    "num_runs = 200  # average over multiple runs\n",
    "num_episodes = 300  # number of episodes for training\n",
    "\n",
    "# Store the first action of each episode\n",
    "q_learning_first_actions = np.zeros((num_runs, num_episodes))\n",
    "double_q_learning_first_actions = np.zeros((num_runs, num_episodes))\n",
    "\n",
    "for i in range(num_runs):\n",
    "    agent = QLearningAgent(env, gamma, epsilon, alpha)\n",
    "    q_learning_first_actions[i] = evaluate_agent(env, agent, num_episodes)\n",
    "\n",
    "    agent = DoubleQLearningAgent(env, gamma, epsilon, alpha)\n",
    "    double_q_learning_first_actions[i] = evaluate_agent(env, agent, num_episodes)\n",
    "\n",
    "# Compute average over the runs\n",
    "q_learning_first_actions = np.mean(q_learning_first_actions, 0)\n",
    "double_q_learning_first_actions = np.mean(double_q_learning_first_actions, 0)\n",
    "\n",
    "# Plot the results\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('% left actions from A')\n",
    "plt.plot(1 - q_learning_first_actions, label='Q-learning')\n",
    "plt.plot(1 - double_q_learning_first_actions, label='Double Q-learning')\n",
    "plt.hlines(y=0.05, xmin=0, xmax=num_episodes, linestyle='--', color='gray', label='Optimal')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
