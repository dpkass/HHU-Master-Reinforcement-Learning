{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD control\n",
    "\n",
    "In this notebook we will implement the TD control algorithms SARSA and Q-learning.  \n",
    "The main difference between them is the TD target, e.g. for SARSA:\n",
    "\n",
    "$\\begin{equation}\n",
    "    Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha (\\,\\underbrace{r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})}_{\\text{TD target}} - Q(s_t, a_t))\n",
    "\\end{equation}$\n",
    "\n",
    "The target of Q-learning can be found in the slides.  \n",
    "Another difference is whether the next action is already selected before the update (only SARSA)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Make sure that the files `rl_agent.py` and `rl_env.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rl_agent\n",
    "import rl_env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents\n",
    "\n",
    "The class `TDAgent` is a base class and uses the method `compute_target()` to compute the TD target.  \n",
    "The two TD agents are subclasses of `TDAgent` and only need to implement this method.  \n",
    "Follow the instructions in the code cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent(rl_agent.TabularAgent):\n",
    "\n",
    "    def __init__(self, env, gamma, epsilon, alpha, rng=None):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.epsilon = epsilon  # epsilon-greedy probability\n",
    "        self.alpha = alpha      # learning rate\n",
    "        self.rng = rng if rng is not None else np.random.default_rng()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create array for action values\n",
    "        self.q = np.zeros((self.num_states, self.num_actions), dtype=float)\n",
    "        # Create array for policy distribution (initialized uniformly)\n",
    "        self.pi = np.full((self.num_states, self.num_actions), 1 / self.num_actions)\n",
    "\n",
    "    def policy(self, state):\n",
    "        # Sample an action according to pi\n",
    "        action_probs = self.pi[state]\n",
    "        action = self.rng.choice(self.num_actions, p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def value(self, state):\n",
    "        # Compute the state value from q and pi\n",
    "        return np.sum(self.pi[state] * self.q[state])\n",
    "\n",
    "    def action_value(self, state, action):\n",
    "        # Lookup the action value in q\n",
    "        return self.q[state, action]\n",
    "\n",
    "    def policy_evaluation(self, state, action, reward, next_state, terminated, truncated):\n",
    "        #######################################################################\n",
    "        # TODO Perform one step of policy evaluation using compute_target().  #\n",
    "        # Update the action value for the given transition. Remember to check #\n",
    "        # if the episode is terminated. Note that SARSA already selects the   #\n",
    "        # next action, so in this case the method should return the next      #\n",
    "        # action, in order to perform it in the environment.                  #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "\n",
    "    def compute_target(self, reward, next_state):\n",
    "        # Compute the TD target and the next action, if necessary.\n",
    "        # Needs to be implemented in a subclass.\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        # Update the policy using epsilon-greedy policy improvement\n",
    "        q = self.q\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        # Find the maximizing actions\n",
    "        max_q = np.max(q, axis=1, keepdims=True)\n",
    "        max_mask = q == max_q\n",
    "        num_max = np.sum(max_mask, axis=1, keepdims=True)\n",
    "\n",
    "        # Compute the probabilities\n",
    "        num_actions = q.shape[1]\n",
    "        pi = ((1 - epsilon) / num_max) * max_mask\n",
    "        pi += epsilon / num_actions\n",
    "        self.pi = pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent(TDAgent):\n",
    "\n",
    "    def compute_target(self, reward, next_state):\n",
    "        #######################################################################\n",
    "        # TODO Calculate the SARSA target. Remember that this algorithm       #\n",
    "        # already selects the next action, so it should be returned as well.  #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(TDAgent):\n",
    "\n",
    "    def compute_target(self, reward, next_state):\n",
    "        #######################################################################\n",
    "        # TODO Calculate the Q-learning target.                               #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Now we evaluate the TD agents on the cliff walking environment.  \n",
    "The goal is to recreate Figure 6.3 from the textbook http://incompleteideas.net/book/the-book-2nd.html  \n",
    "which compares different learning rates $\\alpha$. This week, we will plot the lines for Sarsa and Q-learning only.\n",
    "\n",
    "First implement `train_td_agent()`, which learns a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_td_agent(env, agent, num_episodes):\n",
    "    #######################################################################\n",
    "    # TODO Train a given TD agent by collecting `num_episodes` episodes   #\n",
    "    # in the given environment and performing policy evaluation and       #\n",
    "    # policy improvement.                                                 #\n",
    "    #######################################################################\n",
    "    \n",
    "    #######################################################################\n",
    "    # End of your code.                                                   #\n",
    "    #######################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement `evaluate_td_agent()`, which evaluates a learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_td_agent(env, agent, num_episodes):\n",
    "    #######################################################################\n",
    "    # TODO Evaluate a trained TD agent by collecting `num_episodes`       #\n",
    "    # episodes and calculating the average sum of rewards per episode.    #\n",
    "    #######################################################################\n",
    "    \n",
    "    #######################################################################\n",
    "    # End of your code.                                                   #\n",
    "    #######################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the TD agents in the cliff walking environment.  \n",
    "We want to compare their results when using different learning rates $\\alpha \\in \\{0.1, 0.4, 0.7, 1.0\\}$,  \n",
    "so you need to train and evaluate each agent four times.  \n",
    "Always use $\\gamma = 1.0$ and $\\epsilon = 0.1$, and train and evaluate for $3000$ episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cliff walking environment\n",
    "env = rl_env.cliff_walking()\n",
    "gamma = 1.0    # discount factor\n",
    "epsilon = 0.1  # epsilon-greedy probability\n",
    "alphas = [0.1, 0.4, 0.7, 1.0]  # learning rates\n",
    "num_train_episodes = 3000  # number of episodes for training\n",
    "num_eval_episodes = 3000   # number of episodes for evaluation\n",
    "\n",
    "#######################################################################\n",
    "# TODO Train and evaluate the two agents as described above. Plot   #\n",
    "# the resulting sums of rewards per episode, to recreate Figure 6.3   #\n",
    "# from the textbook.                                                  #\n",
    "#######################################################################\n",
    "\n",
    "#######################################################################\n",
    "# End of your code.                                                   #\n",
    "#######################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
