{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo and Temporal Difference Prediction\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from random import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the environment for generating random walk episodes below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_rollout(n_states=5, left_reward=0, right_reward=1):\n",
    "    terminal_states = [0, n_states-1]\n",
    "    s = n_states//2 # start in the middle\n",
    "    e = [s]   # episode\n",
    "    while s not in terminal_states:\n",
    "        # do random action 'right' or 'left'\n",
    "        s += 1 if random() > 0.5 else -1     # next state\n",
    "        if s == n_states - 1:\n",
    "            r = right_reward\n",
    "        elif s == 0:\n",
    "            r = left_reward\n",
    "        else:\n",
    "            r = 0\n",
    "        e.extend([float(r), s])\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement value iteration for computing value estimates for the episodes. Start with update per episode, and then call the function in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_step_inc(V, e, gamma, alpha):\n",
    "    #######################################################################\n",
    "    # TODO  implement monte carlo prediction update for one episode       #\n",
    "    #######################################################################\n",
    "\n",
    "    #######################################################################\n",
    "    # End of your code.                                                   #\n",
    "    #######################################################################\n",
    "\n",
    "def TD_step(V, e, gamma, alpha):\n",
    "    #######################################################################\n",
    "    # TODO  implement TD prediction update for one episode                #\n",
    "    #######################################################################\n",
    "\n",
    "    #######################################################################\n",
    "    # End of your code.                                                   #\n",
    "    #######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to track how our value estimates converge over time. Complete the code below using the functions you have implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RMSError(v0, v1):\n",
    "    return np.sqrt(((v0-v1)**2).mean())\n",
    "\n",
    "def mc_compute_v(episodes, alpha):\n",
    "\n",
    "    n_episodes = len(episodes)\n",
    "    rms = np.zeros(n_episodes)\n",
    "\n",
    "    for k in range(100):\n",
    "        v = np.zeros(n_states)\n",
    "        \n",
    "        #######################################################################\n",
    "        # TODO call the episode-wise MC update you have implemented above     #\n",
    "        # and maintain a list tracking the RMS error of the estimate          #\n",
    "        #######################################################################\n",
    "            \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "    \n",
    "    rms /= 100\n",
    "    \n",
    "    return v, rms\n",
    "    \n",
    "\n",
    "def td_compute_v(episodes, alpha):\n",
    "    \n",
    "    n_episodes = len(episodes)\n",
    "    rms = np.zeros(n_episodes)\n",
    "    \n",
    "    for k in range(100):\n",
    "        v = np.zeros(n_states)\n",
    "        #######################################################################\n",
    "        # TODO call the episode-wise TD update you have implemented above     #\n",
    "        # and maintain a list tracking the RMS error of the estimate          #\n",
    "        #######################################################################\n",
    "\n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "    \n",
    "    rms /= 100\n",
    "    \n",
    "    return v, rms\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the functions above to answer the questions:\n",
    "* How does MC and TD differs?\n",
    "* What is the influence of alpha on the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = []\n",
    "\n",
    "gamma = 1.0\n",
    "n_states = 5\n",
    "n_episodes = 1000\n",
    "td_alphas=[0.01, 0.02, 0.05]\n",
    "mc_alphas=[0.001, 0.002, 0.003, 0.01]\n",
    "\n",
    "v_true = [0, 1/4, 2/4, 3/4, 0]\n",
    "\n",
    "# rollout episodes\n",
    "for _ in range(n_episodes):\n",
    "    episodes.append(random_walk_rollout(n_states = n_states))\n",
    "\n",
    "# value estimates with MC:\n",
    "print(\"\\n====== MC ======\\n\")\n",
    "for alpha in mc_alphas:\n",
    "    print(f\"alpha:{alpha}\")\n",
    "    v, rms = mc_compute_v(episodes, alpha)\n",
    "    plt.plot(rms, '--', label=f'MC {alpha}')\n",
    "    print(v)\n",
    "\n",
    "# value estimates with TD:\n",
    "print(\"\\n====== TD ======\\n\")\n",
    "for alpha in td_alphas:\n",
    "    print(f\"alpha:{alpha}\")\n",
    "    v, rms = td_compute_v(episodes, alpha)\n",
    "    plt.plot(rms, label=f'TD {alpha}')\n",
    "    print(v)\n",
    "\n",
    "plt.xlabel('walks')\n",
    "plt.ylabel('RMS error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
