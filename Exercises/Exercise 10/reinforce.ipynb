{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. M. Gašić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE\n",
    "\n",
    "In general, the update rule of policy gradient can be written as:\n",
    "$$\n",
    "w \\leftarrow w + \\alpha \\, \\Phi_t \\nabla_w \\log \\pi_w(a_t | s_t)\n",
    "$$\n",
    "where $w$ is the weight vector, $\\alpha$ is the learning rate, and $\\Phi_t \\in \\mathbb{R}$ indicates how good it was to select the action $a_t$ in the state $s_t$.  \n",
    "The default REINFORCE algorithm uses $\\Phi_t = \\gamma^t g_t$, i.e., the discounted return (calculated with a Monte Carlo sample).  \n",
    "\n",
    "As we will see in the lecture, there are other valid options for $\\Phi_t$ that have lower variance.  \n",
    "In this notebook, we will use $\\Phi_t = \\gamma^t (g_t - b)$, where $b$ (baseline) is the average of all returns that we have seen so far.  \n",
    "\n",
    "Your task is to implement the following algorithm.\n",
    "\n",
    "**REINFORCE with average return baseline:**\n",
    "- Initialize counter: $N \\leftarrow 0$\n",
    "- Initialize baseline: $b \\leftarrow 0$\n",
    "- Loop forever:\n",
    "  - Generate an episode $s_0, a_0, r_1, \\ldots, s_{T-1}, a_{T-1}, r_T$ following $\\pi_w$\n",
    "  - Initialize return: $g \\leftarrow 0$\n",
    "  - For $t = T - 1, \\ldots, 0$:  \n",
    "    - Compute return: $g \\leftarrow r_{t + 1} + \\gamma g$\n",
    "    - Increment counter: $N \\leftarrow N + 1$\n",
    "    - Update baseline: $b \\leftarrow b + (g - b) / N$\n",
    "    - Update weights: $w \\leftarrow w + \\alpha \\, \\gamma^t (g - b) \\, \\nabla_w \\log \\pi_w(a_t | s_t)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Policy\n",
    "\n",
    "In this notebook, we will implement a linear softmax policy, i.e.,\n",
    "$$\n",
    "\\pi_w(a | s) = \\frac{\\exp\\left( x(s, a)^\\top w \\right)}{\\sum_{a'} \\exp\\left( x(s, a')^\\top w \\right)},\n",
    "$$\n",
    "where $x(s, a) \\in \\mathbb{R}^d$ is a feature vector of the state and action, and $w \\in \\mathbb{R}^d$ is a weight vector.\n",
    "\n",
    "The score function of this policy, which is required for the REINFORCE algorithm, can be computed with\n",
    "$$\n",
    "\\nabla_w \\log \\pi_w(a | s) = x(s, a) - \\sum_{a'} \\pi(a' | s) \\, x(s, a').\n",
    "$$\n",
    "\n",
    "We will use the following features:\n",
    "$$\n",
    "x(s, a) = [\\ \\underbrace{0, \\ldots, 0}_{D \\,\\cdot\\, a},\\ s, \\underbrace{0, \\ldots, 0}_{D \\,\\cdot\\, (M - a - 1)}]^\\top \\in \\mathbb{R}^{D \\,\\cdot\\, M}\n",
    "$$\n",
    "where $s \\in \\mathbb{R}^D$, $a \\in \\{0, \\ldots, M - 1\\}$.  \n",
    "In other words, we put the entire state vector at a different location depending on the action, and fill the rest with zeros.  \n",
    "This has the effect that $x(s, a)^\\top w$ will use a different set of weights for each action, as the other ones are multiplied by zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Make sure that the files `rl_gui.py` and `rl_tests.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import rl_gui\n",
    "import rl_tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented the features as described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionFeatures:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        # check state space\n",
    "        if not (isinstance(observation_space, gym.spaces.Box) and len(observation_space.shape) == 1):\n",
    "            raise ValueError('Observation space must be a real-valued vector')\n",
    "        \n",
    "        # check action space\n",
    "        if not isinstance(action_space, gym.spaces.Discrete):\n",
    "            raise ValueError('Action space must be discrete')\n",
    "        \n",
    "        self.state_dim = observation_space.shape[0]\n",
    "        self.num_actions = action_space.n\n",
    "\n",
    "    def __call__(self, state, action):\n",
    "        state_dim = self.state_dim\n",
    "        # create vector with zeros\n",
    "        x = np.zeros(state_dim * self.num_actions, dtype=np.float32)\n",
    "        # copy state into its slot\n",
    "        i = action * state_dim\n",
    "        x[i:i + state_dim] = state\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following base class for learned policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # sample an action from pi(. | state)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def score_function(self, state, action):\n",
    "        # compute gradient of log pi(action | state)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update(self, state, action, advantage):\n",
    "        # update the weights for the given state and action, weighted by the advantage\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class implements softmax policies as described above.  \n",
    "The learning rate `alpha` is also part of the policy.  \n",
    "The parameter `feature_fn` is a function that computes a feature vector for a given state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLinearPolicy(Policy):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, alpha, feature_fn, rng=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # learning rate\n",
    "\n",
    "        # check the action space\n",
    "        if not isinstance(action_space, gym.spaces.Discrete):\n",
    "            raise ValueError('Action space must be discrete')\n",
    "        self.num_actions = action_space.n\n",
    "\n",
    "        # function that converts a state and action into a feature vector\n",
    "        self.feature_fn = feature_fn\n",
    "\n",
    "        # determine the size of the feature vectors\n",
    "        # sample a random state and action and convert them to a feature vector\n",
    "        sample_features = feature_fn(observation_space.sample(), action_space.sample())\n",
    "        if not (isinstance(sample_features, np.ndarray) and len(sample_features.shape) == 1):\n",
    "            raise ValueError('Features must be real-valued vectors')\n",
    "\n",
    "        # size of feature vectors\n",
    "        self.feature_dim = sample_features.shape[0]\n",
    "\n",
    "        # initialize random number generator\n",
    "        self.rng = rng if rng is not None else np.random.default_rng()\n",
    "\n",
    "        # reset the weights\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # create weight vector with size of feature vectors, initialize with zeros\n",
    "        self.weights = np.zeros(self.feature_dim, dtype=np.float32)\n",
    "\n",
    "    def compute_probs(self, state):\n",
    "        # utility function that computes the probabilities pi(. | state) for all actions,\n",
    "        # it also returns the computed feature vectors\n",
    "        w = self.weights\n",
    "        num_actions = self.num_actions\n",
    "        xs = []  # list of feature vectors for each action\n",
    "        #######################################################################\n",
    "        # TODO Compute the action probabilities for the given state using the #\n",
    "        # softmax as described above. Use self.feature_fn(state, action) to   #\n",
    "        # compute feature vectors. Also store the feature vectors in the list #\n",
    "        # `xs`, since they can be reused later.                               #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        probs = probs.astype(np.float32, copy=False)\n",
    "        return probs, xs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # sample an action given a state\n",
    "        # compute action probabilities\n",
    "        probs, _ = self.compute_probs(state)\n",
    "        # sample from the distribution\n",
    "        action = np.random.choice(self.num_actions, p=probs)\n",
    "        return action\n",
    "\n",
    "    def score_function(self, state, action):\n",
    "        w = self.weights\n",
    "        num_actions = self.num_actions\n",
    "        #######################################################################\n",
    "        # TODO Compute the score function for the given state and action. Use #\n",
    "        # the function compute_probs() to obtain the probabilities and the    #\n",
    "        # feature vectors.                                                    #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        score = score.astype(np.float32, copy=False)\n",
    "        return score\n",
    "\n",
    "    def update(self, state, action, advantage):\n",
    "        w = self.weights\n",
    "        alpha = self.alpha\n",
    "        #######################################################################\n",
    "        # TODO Update the weight vector as described above. (advantage = phi) #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        self.weights = w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new test with fixed expected scores\n",
    "\n",
    "def test_softmax_policy():\n",
    "    rng = np.random.default_rng(seed=17)\n",
    "    obs_space = gym.spaces.Box(np.zeros(3, dtype=np.float32), np.ones(3, dtype=np.float32))\n",
    "    act_space = gym.spaces.Discrete(4)\n",
    "    features = ActionFeatures(obs_space, act_space)\n",
    "    policy = SoftmaxLinearPolicy(obs_space, act_space, 0.07, features, rng=rng)\n",
    "\n",
    "    sample_state = lambda: rng.uniform(0.0, 1.0, obs_space.shape).astype(np.float32, copy=False)\n",
    "    sample_action = lambda: rng.choice(act_space.n)\n",
    "\n",
    "    yield 'compute_probs()'\n",
    "\n",
    "    policy.weights = rng.standard_normal(policy.weights.shape).astype(np.float32, copy=False)\n",
    "\n",
    "    for expected_probs in [\n",
    "        np.array([0.90269875, 0.01361316, 0.04949653, 0.03419157], dtype=np.float32),\n",
    "        np.array([0.74848235, 0.05044989, 0.12121326, 0.07985448], dtype=np.float32)\n",
    "    ]:\n",
    "        state = sample_state()\n",
    "        probs, xs = policy.compute_probs(state)\n",
    "        if (yield from rl_tests.check_numpy_array(probs, 'probs', shape=expected_probs.shape, dtype=np.float32)):\n",
    "            yield np.allclose(probs, expected_probs), \\\n",
    "                f'Computed probabilities are incorrect (error = {np.sum(np.abs(probs - expected_probs))})'\n",
    "\n",
    "        yield isinstance(xs, list) and len(xs) == act_space.n, \\\n",
    "            f'xs must be a list of length {act_space.n} (got {len(xs)})'\n",
    "        for a, x in enumerate(xs):\n",
    "            yield np.allclose(x, features(state, a)), f'xs contains wrong feature vectors (for action {a})'\n",
    "        yield None\n",
    "\n",
    "    yield 'score_function()'\n",
    "\n",
    "    for expected_score in [\n",
    "        np.array([-0.0542655,  -0.04997427, -0.13998121,  0.14724284,  0.13559912,  0.37982202, -0.03655295, -0.0336624,  -0.09429058, -0.05642439, -0.05196244, -0.1455502 ], dtype=np.float32),\n",
    "        np.array([-0.31696385, -0.64511997, -0.5715783,  -0.02204133, -0.04486096, -0.03974695, 0.39505485,  0.8040593,   0.7123991,  -0.05604962, -0.1140784,  -0.10107382], dtype=np.float32)\n",
    "    ]:\n",
    "        score = policy.score_function(sample_state(), sample_action())\n",
    "        if (yield from rl_tests.check_numpy_array(score, 'score', shape=expected_score.shape, dtype=np.float32)):\n",
    "            yield np.allclose(score, expected_score), \\\n",
    "                f'Computed score is incorrect (error = {np.sum(np.abs(score - expected_score))})'\n",
    "        yield None\n",
    "\n",
    "    yield 'update()'\n",
    "\n",
    "    for expected_weights in [\n",
    "        np.array([-0.20219066, -2.27896578,  2.03422714, -2.17468828, -2.08132293, -1.2758126, 0.55199572,  1.78623307, -0.24874753, -0.38633042,  0.17751255, -0.36193669], dtype=np.float32),\n",
    "        np.array([-0.1025072,   0.16325331,  0.56471321, -0.81238355, -1.32787222, -0.94905908, 0.52641521, -0.85278105, -0.77391747,  1.15396637, -0.78571158, -0.89402242], dtype=np.float32)\n",
    "    ]:\n",
    "        policy.weights = rng.standard_normal(policy.weights.shape)\n",
    "        policy.update(sample_state(), sample_action(), rng.uniform(-1.0, 1.0))\n",
    "        yield np.allclose(policy.weights, expected_weights), \\\n",
    "            f'Updated weights are incorrect (error = {np.sum(np.abs(policy.weights - expected_weights))})'\n",
    "        yield None\n",
    "\n",
    "rl_tests.run_tests(test_softmax_policy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We will evaluate the algorithm on the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(seed):\n",
    "    env_id = f'CartPole-v1'\n",
    "    env = gym.make(env_id, render_mode='rgb_array')\n",
    "    env.reset(seed=seed)\n",
    "    return env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env(seed=42)\n",
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'sum of rewards: {reward_sum:.1f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the policy using REINFORCE.  \n",
    "You can play around with the hyperparameters and try to find a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.00015      # learning rate\n",
    "gamma = 1.0          # discount factor\n",
    "num_episodes = 5000  # number of episodes to train\n",
    "\n",
    "# create the environment\n",
    "env = create_env(seed=42)\n",
    "# create the feature function\n",
    "features = ActionFeatures(env.observation_space, env.action_space)\n",
    "# create the softmax policy\n",
    "policy = SoftmaxLinearPolicy(env.observation_space, env.action_space, alpha, features)\n",
    "\n",
    "# store the sum of rewards per episode for plotting\n",
    "reward_sums = []\n",
    "\n",
    "# REINFORCE algorithm starts here\n",
    "# initialize counter and baseline\n",
    "count = 0\n",
    "baseline = 0.0\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    # generate an episode\n",
    "    state, _ = env.reset()\n",
    "    states = [state]\n",
    "    rewards = [0.0]\n",
    "    actions = []\n",
    "    while True:\n",
    "        # select action using the policy\n",
    "        action = policy.select_action(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # store the initial return (for plotting)\n",
    "    reward_sums.append(sum(rewards))\n",
    "\n",
    "    # update the policy using the episode\n",
    "    T = len(actions)\n",
    "    g = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        #######################################################################\n",
    "        # TODO Compute the return and update the counter and baseline as      #\n",
    "        # described in the algorithm at the top of this notebook.             #\n",
    "        # Then compute the advantage (= phi), which is used to update the     #\n",
    "        # weights in the line below your code.                                #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        # update weights\n",
    "        policy.update(states[t], actions[t], advantage)\n",
    "\n",
    "plt.title('CartPole')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Sum of rewards')\n",
    "plt.plot(reward_sums, c='C0', alpha=0.3)\n",
    "plt.plot(np.convolve(reward_sums, np.ones(100) / 100, mode='valid'), c='C0');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of training, the sum of rewards should be around 500, but it might be very noisy.  \n",
    "Evaluate the learned policy (run again if you have bad luck):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "state, _ = env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "\n",
    "while True:\n",
    "    action = policy.select_action(state)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'action: {action}, sum of rewards: {reward_sum:.1f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
