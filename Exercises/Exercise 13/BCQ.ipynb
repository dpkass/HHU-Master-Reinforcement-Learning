{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Offline Reinforcement Learning\n",
    "\n",
    "In offline RL, we are given a dataset of transitions\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{ (s_n, a_n, r_n, t_n, s^\\prime_{n}) \\}_n\n",
    "$$\n",
    "\n",
    "consisting of states, actions, rewards, termination signals and next states. The dataset was collected by a potentially unknown behaviour policy $\\mu$. Our goal is to learn a policy $\\pi$ using $\\mathcal{D}$ without generating new data.\n",
    "\n",
    "Off-policy algorithms such as DQN are able to learn from data that was produced by a different policy. In this exercise, we will first investigate whether dueling DQN (DDQN) is able to learn from offline data.\n",
    "\n",
    "Subsequently, we will implement a generalized version called batch-constrained Q-learning (BCQ) that was constructed for the task of offline RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Implementation\n",
    "\n",
    "Make sure that the files `rl_gui.py` and `rl_tests.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-01-29T15:10:59.539648Z",
     "start_time": "2025-01-29T15:10:54.668048Z"
    }
   },
   "source": [
    "import collections\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import rl_gui\n",
    "import rl_tests"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-01-29T15:10:59.543873Z",
     "start_time": "2025-01-29T15:10:59.540806Z"
    }
   },
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity, rng):\n",
    "        # create a queue that removes old transitions when capacity is reached\n",
    "        self.transitions = collections.deque([], maxlen=capacity)\n",
    "\n",
    "        # random number generator used for sampling batches\n",
    "        self.rng = rng\n",
    "\n",
    "    def append(self, transition):\n",
    "        # append a transition (a tuple) to the queue\n",
    "        self.transitions.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # randomly sample a list of indices\n",
    "        idx = self.rng.choice(len(self.transitions), batch_size, replace=False)\n",
    "\n",
    "        # select the transitions using the indices\n",
    "        transitions = [self.transitions[i] for i in idx]\n",
    "\n",
    "        batches = tuple(torch.as_tensor(np.array(batch)) for batch in zip(*transitions))\n",
    "        return batches\n",
    "\n",
    "    def save_transitions(self, suffix=\"\"):\n",
    "        torch.save(self.transitions, f\"transitions_{suffix}\")\n",
    "\n",
    "    def load_transitions(self, path_name):\n",
    "        self.transitions = torch.load(path_name)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We already implemented the dueling DQN algorithm that we saw in Exercise 9."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-01-29T15:10:59.550682Z",
     "start_time": "2025-01-29T15:10:59.544664Z"
    }
   },
   "source": [
    "class DDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, num_actions, learning_rate, gamma):\n",
    "        super().__init__()\n",
    "        # create a simple neural network with two fully-connected layers\n",
    "        # and a ReLU nonlinearity\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_q(self, states, actions):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "        # actions has shape (batch_size)\n",
    "\n",
    "        # compute q[s], which has shape (batch_size, num_actions)\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        # select q[s,a], which has shape (batch_size)\n",
    "        q = torch.gather(q_all, dim=1, index=actions.long().unsqueeze(1)).squeeze(1)\n",
    "        return q\n",
    "\n",
    "    def compute_max_q(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "\n",
    "        # compute q[s], which has shape (batch_size, num_actions)\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        # select max_a' q[s,a'], which has shape (batch_size)\n",
    "        max_q = q_all.max(dim=1)[0]\n",
    "        return max_q\n",
    "\n",
    "    def compute_arg_max(self, states):\n",
    "        # states has shape (batch_size, state_dim)\n",
    "\n",
    "        # compute q[s], which has shape (batch_size, num_actions)\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        # select argmax_a' q[s,a'], which has shape (batch_size)\n",
    "        actions = q_all.argmax(dim=1)\n",
    "        return actions\n",
    "\n",
    "    def compute_loss(self, target_dqn, batches):\n",
    "        states, actions, rewards, terminations, next_states = batches\n",
    "\n",
    "        # turn off gradient computation\n",
    "        with torch.no_grad():\n",
    "            arg_max = self.compute_arg_max(next_states)\n",
    "            targets = target_dqn.compute_q(next_states, arg_max)\n",
    "            targets = rewards + self.gamma * (terminations != 1).float() * targets\n",
    "\n",
    "        # compute predictions q[s,a]\n",
    "        q = self.compute_q(states, actions)\n",
    "\n",
    "        # compute mean squared error between q[s,a] and targets\n",
    "        loss = torch.mean((q - targets.detach()) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def update(self, memory, batch_size, target_dqn):\n",
    "        batches = memory.sample(batch_size)\n",
    "        # minimize the loss function using SGD\n",
    "        self.train()  # switch to training mode\n",
    "        loss = self.compute_loss(target_dqn, batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-01-29T15:10:59.554344Z",
     "start_time": "2025-01-29T15:10:59.551350Z"
    }
   },
   "source": [
    "# helper function for generating datasets\n",
    "def generate_data(dqn, env, epsilon, num_transitions, rng):\n",
    "    replay_memory = ReplayMemory(num_transitions, rng=rng)\n",
    "\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for _ in range(num_transitions):\n",
    "\n",
    "        # epsilon-greedy policy\n",
    "        if rng.random() < epsilon:\n",
    "            action = rng.randint(env.action_space.n)\n",
    "        else:\n",
    "            action = dqn.compute_arg_max(torch.as_tensor(state).unsqueeze(0)).item()\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # store transition in replay memory\n",
    "        replay_memory.append((state, action, reward, terminated, next_state))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            state, _ = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    return replay_memory"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Implement the `train_offline` and `evaluate_agent` function.\n",
    "\n",
    "The `train_offline` method performs offline RL using a replay memory.\n",
    "\n",
    "The `evaluate_agent` method runs $n$ episodes. In each episode $i$ it should calculate the sum of rewards $g_{0, i}$ and the approximated value of the starting state $V(s_0)_i$ (given by the maximum of the Q-function). It then returns the average over all episodes of these two quantities, i.e.\n",
    "\n",
    "$$\n",
    "1/n \\sum_{i=1}^{n} g_{0, i}, \\quad 1/n \\sum_{i=1}^{n} V(s_0)_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-01-29T15:17:00.328605Z",
     "start_time": "2025-01-29T15:17:00.320277Z"
    }
   },
   "source": [
    "def train_offline(dqn, target_dqn, target_interval, memory, num_updates, batch_size):\n",
    "    #######################################################################\n",
    "    # TODO Perform num_updates many updates of the dqn agent.             #\n",
    "    # use the \"update()\" function of DDQN. Update the target_dqn after    #\n",
    "    # target_interval many update steps by using copy.deepcopy()          #\n",
    "    #######################################################################\n",
    "\n",
    "    for i in range(num_updates):\n",
    "        dqn.update(memory, batch_size, target_dqn)\n",
    "        if i % target_interval == 0 and i > 0:\n",
    "            target_dqn = copy.deepcopy(dqn)\n",
    "\n",
    "    return target_dqn\n",
    "\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "def evaluate_agent(env, agent, num_episodes):\n",
    "    '''\n",
    "    :param env: environment for interaction\n",
    "    :param agent: RL agent\n",
    "    :param num_episodes: Run num_episodes many episodes\n",
    "    :return: the average return across episodes and the average value estimates of the starting states\n",
    "    '''\n",
    "\n",
    "    #######################################################################\n",
    "    # TODO Run num_episodes episodes and compute the average return and   #\n",
    "    # the average value estimates of the starting states.                 #\n",
    "    # Use argmax for action selection.                                    #\n",
    "    #######################################################################\n",
    "\n",
    "    avg_return = avg_value = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.as_tensor(state).unsqueeze(0)\n",
    "        avg_value += agent.compute_max_q(state).item()\n",
    "\n",
    "        for t in count(0):\n",
    "            action = agent.compute_arg_max(state).item()\n",
    "            state, reward, terminated, *_ = env.step(action)\n",
    "            avg_return += reward\n",
    "            if terminated: break\n",
    "            state = torch.as_tensor(state).unsqueeze(0)\n",
    "\n",
    "    avg_return /= num_episodes\n",
    "    avg_value /= num_episodes\n",
    "\n",
    "    return avg_return, avg_value"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:17:00.698052Z",
     "start_time": "2025-01-29T15:17:00.695781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_env(seed):\n",
    "    env_id = 'LunarLander-v3'\n",
    "    #env_id = \"ALE/MsPacman-v5\"\n",
    "    env = gym.make(env_id, render_mode='rgb_array')\n",
    "    env.reset(seed=seed)\n",
    "    return env"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-01-29T15:17:01.636926Z",
     "start_time": "2025-01-29T15:17:01.007729Z"
    }
   },
   "source": [
    "def test_train():\n",
    "    rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "    torch.manual_seed(42)\n",
    "    env = create_env(42)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = DDQN(env.observation_space.shape[0], env.action_space.n, learning_rate=0.1, gamma=0.99)\n",
    "    target_agent = copy.deepcopy(agent)\n",
    "    memory = generate_data(agent, env, 0.0, 12, rng)\n",
    "\n",
    "    sample_states = lambda batch_size: torch.as_tensor(rng.standard_normal((batch_size, state_dim), dtype=np.float32))\n",
    "    sample_actions = lambda batch_size: torch.as_tensor(rng.choice(action_dim, batch_size))\n",
    "\n",
    "    yield \"train_offline()\"\n",
    "\n",
    "    for expected_q_values in [\n",
    "        [0.14601666, 0.20298843, 0.07980790, 0.20461105, 0.47405303],\n",
    "        [0.32259068, -2.07440472, 0.01698574, -14.59191322, -2.82291675]\n",
    "    ]:\n",
    "        states = sample_states(5)\n",
    "        actions = sample_actions(5)\n",
    "        target_agent = train_offline(agent, target_agent, 2, memory, 5, 5)\n",
    "        q_values = agent.compute_q(states, actions).detach()\n",
    "\n",
    "        yield torch.allclose(q_values, torch.as_tensor(\n",
    "            expected_q_values)), f'Q-values are incorrect (error = {(torch.abs(q_values - torch.as_tensor(expected_q_values))).sum().item()})'\n",
    "        yield None\n",
    "\n",
    "\n",
    "def test_evaluate():\n",
    "    torch.manual_seed(42)\n",
    "    env = create_env(42)\n",
    "    agent = DDQN(env.observation_space.shape[0], env.action_space.n, learning_rate=0.1, gamma=0.99)\n",
    "\n",
    "    yield \"evaluate_agent()\"\n",
    "    num_episodes = [1, 3, 5]\n",
    "    avg_returns = [-591.34799665, -2014.67668242, -1145.47214997]\n",
    "    avg_values = [0.13210351, 0.21220374, 0.16889345]\n",
    "\n",
    "    for num, ret, val in zip(num_episodes, avg_returns, avg_values):\n",
    "        avg_return, avg_value = evaluate_agent(env, agent, num)\n",
    "\n",
    "        yield torch.allclose(torch.Tensor([ret]), torch.Tensor([avg_return])), f'Average return is incorrect, (error = {abs(ret - avg_return)})'\n",
    "        yield torch.allclose(torch.Tensor([val]), torch.Tensor([avg_value])), f'Average value is incorrect, (error = {abs(val - avg_value)})'\n",
    "\n",
    "        yield None\n",
    "\n",
    "\n",
    "rl_tests.run_tests(test_train())\n",
    "rl_tests.run_tests(test_evaluate())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing train_offline()...\n",
      "1/2 tests passed!\n",
      "Test #2 failed: Q-values are incorrect (error = 4.1093677282333374e-05)\n",
      "Testing evaluate_agent()...\n",
      "3/3 tests passed!\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### We already collected data using the following expert agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "env = create_env(seed=1)\n",
    "render = rl_gui.create_renderer(env, fps=60, figsize=(4, 3))\n",
    "\n",
    "dqn = DDQN(env.observation_space.shape[0], env.action_space.n, learning_rate=0.1, gamma=0.99)\n",
    "dqn.load_state_dict(torch.load('data_and_models/expert_dqn.pt'))\n",
    "\n",
    "state, _ = env.reset()\n",
    "render()\n",
    "reward_sum = 0.0\n",
    "for _ in range(300):\n",
    "    action = dqn.compute_arg_max(torch.as_tensor(state).unsqueeze(0)).item()\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    render(f'sum of rewards: {reward_sum:.2f}')\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### We collected 3 datasets with 100, 1000, and 10000 transitions.\n",
    "\n",
    "Let us run offline RL with the DDQN agent using the implemented `train_offline` method. We evaluate the trained agent every 10000 updates using the `evaluate_agent` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "rng = np.random.default_rng(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "env = create_env(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "batch_size = 64  # number of transitions in a batch\n",
    "replay_capacity = int(1e3)  # number of transitions that are stored in memory, does not matter here\n",
    "gamma = 0.99  # discount factor\n",
    "learning_rate = 0.0001  # learning rate for DDQN\n",
    "target_interval = 100  # synchronize the target network after this number of steps\n",
    "num_offline_updates = 10000  # number of offline updates to perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_paths = [\"transitions_expert_0.1k\", \"transitions_expert_1k\", \"transitions_expert_10k\"]\n",
    "data_paths = [os.path.join(\"data_and_models\", path) for path in data_paths]\n",
    "\n",
    "average_returns_all = []\n",
    "q_values_all = []\n",
    "\n",
    "for path in data_paths:\n",
    "    memory = ReplayMemory(replay_capacity, rng)\n",
    "    memory.load_transitions(path)\n",
    "    print(f\"Number of transitions in {path} dataset: {len(memory.transitions)}\")\n",
    "\n",
    "    ddqn = DDQN(state_dim, num_actions, learning_rate, gamma)\n",
    "    average_returns = []\n",
    "    q_values = []\n",
    "\n",
    "    print(\"Start training...\")\n",
    "\n",
    "    for i in range(20):\n",
    "        # Train for num_offline_updates many steps\n",
    "        target_ddqn = copy.deepcopy(ddqn)\n",
    "        train_offline(ddqn, target_ddqn, target_interval, memory,\n",
    "                      num_offline_updates, batch_size)\n",
    "\n",
    "        # Evaluate the agent\n",
    "        performance, qs = evaluate_agent(env, ddqn, 20)\n",
    "        average_returns.append(performance)\n",
    "        q_values.append(qs)\n",
    "        print(f\"Iteration: {i + 1}/20, Average return: {performance}, Value-Estimates: {qs}\")\n",
    "\n",
    "    average_returns_all.append(average_returns)\n",
    "    q_values_all.append(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's plot the results for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "labels = [\"0.1k Expert data\", \"1k Expert data\", \"10k Expert data\"]\n",
    "\n",
    "x = [(i + 1) for i in range(20)]\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, sharey=False, figsize=(10, 4))\n",
    "\n",
    "axes[0].set_ylabel('Average Return')\n",
    "axes[0].set_ylim([-700, 0])\n",
    "axes[0].set_xlabel('Num updates / 10k')\n",
    "axes[0].set_xticks(x)\n",
    "for i, n in enumerate(average_returns_all):\n",
    "    axes[0].plot(x, n, label=labels[i])\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_ylabel('Value-Estimates')\n",
    "axes[1].set_ylim([0, 5000])\n",
    "axes[1].set_xlabel('Num updates / 10k')\n",
    "axes[1].set_xticks(x)\n",
    "for i, n in enumerate(q_values_all):\n",
    "    axes[1].plot(x, n, label=labels[i])\n",
    "axes[1].legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### What can you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### BCQ\n",
    "\n",
    "Batch-constrained Q-learning tries to overcome the overestimations that result from the target\n",
    "\n",
    "$$\n",
    "r_t + \\gamma \\cdot max_{a^\\prime} Q(s_{t+1}, a^\\prime)\n",
    "$$\n",
    "if actions $a^\\prime$ are out of distribution, i.e. $(s_{t+1}, a^\\prime)$ is not in the dataset and can thus be not updated.\n",
    "\n",
    "The idea is to only allow actions that were likely used by the behaviour policy $\\mu$ that created the dataset. Since these actions are likely to be in the dataset, the overestimation is reduced since we obtain more reliable estimates of the targets.\n",
    "\n",
    "In BCQ, in addition to the Q-network, we learn an imitator network $G_\\omega$ that tries to imitate the behaviour policy $\\mu$. The imitator network is trained to maximize likelihood of the actions in the dataset by minimizing the loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{imitator} = -\\mathbb{E}_{(s, a) \\sim D} \\log G_\\omega(s, a).\n",
    "$$\n",
    "\n",
    "The BCQ agent selects actions, given a state $s$, according to\n",
    "\n",
    "$$\n",
    "\\pi(s) = argmax_{a | G_\\omega(s,a) / max_{a^\\prime} G_\\omega(s,a^\\prime)) \\geq \\tau} \\text{ } Q_\\theta (s, a),\n",
    "$$\n",
    "where $\\tau$ is a threshold value. We thus only consider actions that are \"reliable\" according to the imitator network. The Q-network parameterized by $\\theta$ aims to minimize\n",
    "\n",
    "$$\n",
    "r + \\gamma Q_{\\theta^\\prime}(s^\\prime, a^\\prime) - Q_\\theta(s, a), \\quad a^\\prime = argmax_{a^\\prime | G_\\omega(s^\\prime,a^\\prime) / max_{a^\\prime} G_\\omega(s^\\prime,a^\\prime)) \\geq \\tau} \\text{ } Q_\\theta (s^\\prime, a^\\prime),\n",
    "$$\n",
    "where $\\theta^\\prime$ are the parameters of the target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BCQ(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, num_actions, learning_rate, gamma, threshold=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "        self.imitator = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def compute_log_probs(self, states):\n",
    "        #############################################################################\n",
    "        # TODO Compute the log-probabilities for every state in states using        #\n",
    "        # the imitator network and the torch function logsumexp (as in Exercise 11) #\n",
    "        # the input has shape (batch_size, state_dim)                               #\n",
    "        # the output has shape (batch_size, num_actions)                            #\n",
    "        #############################################################################\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def imitator_loss(self, batches):\n",
    "        #############################################################################\n",
    "        # TODO Calculate the imitator loss L_imitator as described above            #\n",
    "        #                                                                           #\n",
    "        #############################################################################\n",
    "\n",
    "        states, actions, _, _, _ = batches\n",
    "\n",
    "        loss = 0\n",
    "        return loss\n",
    "\n",
    "    def compute_q(self, states, actions):\n",
    "        q_all = self.network(states)\n",
    "        q = torch.gather(q_all, dim=1, index=actions.long().unsqueeze(1)).squeeze(1)\n",
    "        return q\n",
    "\n",
    "    def compute_loss(self, target_dqn, batches):\n",
    "        states, actions, rewards, terminations, next_states = batches\n",
    "\n",
    "        # turn off gradient computation\n",
    "        with torch.no_grad():\n",
    "            arg_max = self.compute_arg_max(next_states)\n",
    "            targets = target_dqn.compute_q(next_states, arg_max)\n",
    "            targets = rewards + self.gamma * (terminations != 1).float() * targets\n",
    "\n",
    "        # compute predictions q[s,a]\n",
    "        q = self.compute_q(states, actions)\n",
    "\n",
    "        # compute mean squared error between q[s,a] and targets\n",
    "        loss = torch.mean((q - targets.detach()) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def compute_max_q(self, states):\n",
    "        ##################################################################################\n",
    "        # TODO Calculate the max q value for every state                                 #\n",
    "        # The max operator should be only applied to actions that satisfy the            #\n",
    "        # constraint mentioned above, i.e. over the set                                  #\n",
    "        # {a | G_\\omega(s,a) / max_{a^\\prime} G_\\omega(s,a^\\prime)) \\geq self.threshold} #\n",
    "        ##################################################################################\n",
    "\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        # select max_a' q[s,a'], which has shape (batch_size)\n",
    "        max_q = q_all.max(dim=1)[0]\n",
    "        return max_q\n",
    "\n",
    "    def compute_arg_max(self, states):\n",
    "        ##################################################################################\n",
    "        # TODO Calculate the arg max q value for every state                             #\n",
    "        # The max operator should be only applied to actions that satisfy the            #\n",
    "        # constraint mentioned above, i.e. over the set                                  #\n",
    "        # {a | G_\\omega(s,a) / max_{a^\\prime} G_\\omega(s,a^\\prime)) \\geq self.threshold} #\n",
    "        ##################################################################################\n",
    "\n",
    "        q_all = self.network(states)\n",
    "\n",
    "        actions = q_all.argmax(dim=1)\n",
    "        return actions\n",
    "\n",
    "    def update(self, memory, batch_size, target_dqn):\n",
    "        batches = memory.sample(batch_size)\n",
    "        # minimize the loss function using SGD\n",
    "        self.train()  # switch to training mode\n",
    "        loss = self.compute_loss(target_dqn, batches) + self.imitator_loss(batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def test_bcq():\n",
    "    torch.manual_seed(42)\n",
    "    rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "    state_dim = 5\n",
    "    num_actions = 3\n",
    "    gamma = 0.8\n",
    "    learning_rate = 0.1\n",
    "    bcq = BCQ(state_dim, num_actions, learning_rate, gamma, threshold=0.3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for layer in (bcq.network[0], bcq.network[2], bcq.imitator[0], bcq.imitator[2]):\n",
    "            mean = rng.uniform(-0.5, 0.5)\n",
    "            layer.weight[:] = torch.as_tensor(rng.normal(mean, 0.1, layer.weight.shape))\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    sample_states = lambda batch_size: torch.as_tensor(rng.standard_normal((batch_size, state_dim), dtype=np.float32))\n",
    "    sample_actions = lambda batch_size: torch.as_tensor(rng.choice(num_actions, batch_size))\n",
    "\n",
    "    yield 'compute_log_probs()'\n",
    "\n",
    "    for expected_log_softmax in [\n",
    "        [[-1.29940438, -0.79732537, -1.28455567],\n",
    "         [-1.01962435, -1.08546841, -1.19889891]],\n",
    "\n",
    "        [[-1.08598757, -1.12744570, -1.08301783],\n",
    "         [-1.10690618, -1.07192361, -1.11757934]]\n",
    "    ]:\n",
    "        batch_size = len(expected_log_softmax)\n",
    "        states = sample_states(batch_size)\n",
    "        log_softmax = bcq.compute_log_probs(states)\n",
    "        yield torch.allclose(log_softmax, torch.as_tensor(\n",
    "            expected_log_softmax)), f'log_softmaxs are incorrect (error = {torch.sum(torch.abs(log_softmax - torch.as_tensor(expected_log_softmax))).item()}'\n",
    "        yield None\n",
    "\n",
    "    yield 'imitator_loss()'\n",
    "\n",
    "    for expected_loss in [\n",
    "        1.097770094871521,\n",
    "        1.147094488143921,\n",
    "        1.0680086612701416\n",
    "    ]:\n",
    "        states = sample_states(5)\n",
    "        actions = sample_actions(5)\n",
    "        batch = (states, actions, _, _, _)\n",
    "        loss = bcq.imitator_loss(batch)\n",
    "        yield torch.allclose(loss, torch.Tensor([expected_loss])), f'Imitator loss is incorrect (error = {torch.sum(torch.abs(loss - expected_loss)).item()}'\n",
    "        yield None\n",
    "\n",
    "    yield 'compute_max_q()'\n",
    "\n",
    "    for expected_value in [\n",
    "        [4.21411991, 0.00000000, 28.97923660, 38.94561005, 33.75335693],\n",
    "        [35.10310364, 15.52004814, 1.47012889, 37.41002274, 14.30604076]\n",
    "    ]:\n",
    "        states = sample_states(5)\n",
    "        value = bcq.compute_max_q(states)\n",
    "\n",
    "        yield torch.allclose(value, torch.as_tensor(\n",
    "            expected_value)), f'Values are incorrect (error = {torch.sum(torch.abs(value - torch.as_tensor(expected_value))).item()}'\n",
    "        yield None\n",
    "\n",
    "    yield 'compute_argmax_q'\n",
    "\n",
    "    for expected_actions in [\n",
    "        [2, 0, 2, 2, 2],\n",
    "        [0, 2, 2, 1, 2]\n",
    "    ]:\n",
    "        states = sample_states(5)\n",
    "        actions = bcq.compute_arg_max(states)\n",
    "        yield torch.all(actions == torch.as_tensor(expected_actions)).item(), 'actions are incorrect'\n",
    "        yield None\n",
    "\n",
    "\n",
    "rl_tests.run_tests(test_bcq())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We run BCQ agents with a threshold of $0.3$ on the datasets with 100 and 1000 transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_paths = [\"transitions_expert_0.1k\", \"transitions_expert_1k\"]\n",
    "data_paths = [os.path.join(\"data_and_models\", path) for path in data_paths]\n",
    "\n",
    "average_returns_all_bcq = []\n",
    "q_values_all_bcq = []\n",
    "\n",
    "for path in data_paths:\n",
    "    memory = ReplayMemory(replay_capacity, rng)\n",
    "    memory.load_transitions(path)\n",
    "    print(f\"Number of transitions in {path}: {len(memory.transitions)}\")\n",
    "\n",
    "    bcq = BCQ(state_dim, num_actions, learning_rate, gamma, threshold=0.3)\n",
    "    average_returns = []\n",
    "    q_values = []\n",
    "\n",
    "    print(\"Start training...\")\n",
    "\n",
    "    for i in range(10):\n",
    "        # Train for num_offline_updates many steps\n",
    "        target_bcq = copy.deepcopy(bcq)\n",
    "        train_offline(bcq, target_bcq, target_interval, memory,\n",
    "                      num_offline_updates, batch_size)\n",
    "\n",
    "        # Evaluate the agent\n",
    "        performance, qs = evaluate_agent(env, bcq, 20)\n",
    "        average_returns.append(performance)\n",
    "        q_values.append(qs)\n",
    "        print(f\"Iteration: {i + 1}/10, Average return: {performance}, Value-Estimates: {qs}\")\n",
    "\n",
    "    average_returns_all_bcq.append(average_returns)\n",
    "    q_values_all_bcq.append(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "labels = [\"0.1k Expert data\", \"1k Expert data\"]\n",
    "\n",
    "x = [(i + 1) for i in range(10)]\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, sharey=False, figsize=(10, 4))\n",
    "\n",
    "axes[0].set_ylabel('Average return')\n",
    "axes[0].set_ylim([-300, 250])\n",
    "axes[0].set_xlabel('Num updates / 10k')\n",
    "axes[0].set_xticks(x)\n",
    "for i, n in enumerate(average_returns_all_bcq):\n",
    "    axes[0].plot(x, n, label=labels[i])\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_ylabel('Value-Estimates')\n",
    "axes[1].set_ylim([-300, 250])\n",
    "axes[1].set_xlabel('Num updates / 10k')\n",
    "axes[1].set_xticks(x)\n",
    "for i, n in enumerate(q_values_all_bcq):\n",
    "    axes[1].plot(x, n, label=labels[i])\n",
    "axes[1].legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### What can you observe when you compare the performance and corresponding Value-estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Lastly, we train a BCQ agent with a dataset that was collect with the sub-optimal policy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "suboptimal_ddqn = DDQN(state_dim, num_actions, learning_rate, gamma)\n",
    "suboptimal_ddqn.load_state_dict(torch.load(\"data_and_models/suboptimal_dqn.pt\"))\n",
    "\n",
    "performance, qs = evaluate_agent(env, suboptimal_ddqn, 20)\n",
    "print(f\"Performance: {performance}, Value-Estimates: {qs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "memory = ReplayMemory(replay_capacity, rng)\n",
    "memory.load_transitions(\"data_and_models/transitions_suboptimal_1k_eps0.2\")\n",
    "print(\"Number of transitions in memory:\", len(memory.transitions))\n",
    "\n",
    "for threshold in [0.3]:\n",
    "\n",
    "    bcq = BCQ(state_dim, num_actions, learning_rate, gamma, threshold=threshold)\n",
    "    target_bcq = copy.deepcopy(bcq)\n",
    "\n",
    "    for i in range(5):\n",
    "        target_bcq = copy.deepcopy(bcq)\n",
    "        train_offline(bcq, target_bcq, target_interval, memory,\n",
    "                      num_updates=10000, batch_size=batch_size)\n",
    "\n",
    "        performance, qs = evaluate_agent(env, bcq, 20)\n",
    "        print(i + 1, performance, qs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Can the Offline Agent outperform the Behaviour policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
