{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Prof. Milica Gašić"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo control\n",
    "\n",
    "1. Policy evaluation by every-visit Monte Carlo prediction of $q_\\pi$:\n",
    "  - Sample an episode $s_0, a_0, r_1, \\ldots, s_T$\n",
    "  - Set final return $g_T$ = 0\n",
    "  - For $t = T - 1$ to $0$:\n",
    "    - Compute return: $g_t = r_{t+1} + \\gamma g_{t+1}$\n",
    "    - Increment total return: $G(s_t, a_t) \\mathrel{+}= g_t$\n",
    "    - Increment counter: $N(s_t, a_t) \\mathrel{+}= 1$\n",
    "    - Update value: $Q(s_t, a_t) = G(s_t, a_t) / N(s_t, a_t)$\n",
    "2. $\\epsilon$-greedy policy improvement:\n",
    "\n",
    "    $\\pi'(a|s) = \\begin{cases}\n",
    "      \\frac{\\epsilon}{m} + \\frac{1 - \\epsilon}{|M_\\pi(s)|} & \\text{if } a \\in M_\\pi(s) \\\\\n",
    "      \\frac{\\epsilon}{m} & \\text{otherwise}\n",
    "    \\end{cases}$\n",
    "    \n",
    "    where $M_\\pi(s) = \\{ a \\,|\\, q_\\pi(s,a) = \\max_{a'} q_\\pi(s,a') \\}$ is the set of actions with maximal action values for state $s$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Make sure that the files `rl_agent.py`, `rl_env.py`, `rl_gui.py`, `rl_tests.py` and `rl_util.py` are in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import rl_agent\n",
    "import rl_env\n",
    "import rl_gui\n",
    "import rl_tests\n",
    "import rl_util"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement value iteration in the class `ValueIterationAgent` below.  \n",
    "Follow the instructions in the method `update_v()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloAgent(rl_agent.TabularAgent):\n",
    "\n",
    "    def __init__(self, env, gamma, epsilon, rng=None):\n",
    "        super().__init__(env)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.rng = rng if rng is not None else np.random.default_rng()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        num_s = self.num_states\n",
    "        num_a = self.num_actions\n",
    "\n",
    "        # Create arrays for total returns, visit counters and action value\n",
    "        self.G = np.zeros((num_s, num_a), dtype=float)\n",
    "        self.N = np.zeros((num_s, num_a), dtype=int)\n",
    "        self.q = np.zeros((num_s, num_a), dtype=float)\n",
    "\n",
    "        # Create array for policy distribution (initialized uniformly)\n",
    "        self.pi = np.full((num_s, num_a), 1 / num_a, dtype=float)\n",
    "\n",
    "        # State values are only computed when need (and are not required for this algorithm)\n",
    "        self.v = None\n",
    "\n",
    "        # Store some statistics for logging\n",
    "        self.num_steps = 0\n",
    "        self.num_episodes = 0\n",
    "        self.num_truncated = 0\n",
    "\n",
    "    def policy(self, state):\n",
    "        # Sample an action according to pi\n",
    "        action_probs = self.pi[state]\n",
    "        action = self.rng.choice(self.num_actions, p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def value(self, state):\n",
    "        # Compute the state value from q and pi\n",
    "        if self.v is None:\n",
    "            self.v = np.sum(self.pi * self.q, axis=1)\n",
    "        return self.v[state]\n",
    "\n",
    "    def update_q(self, episode):\n",
    "        # Update the action values given an episode\n",
    "\n",
    "        if episode['truncated']:\n",
    "            # If the episode was truncated, we cannot use it,\n",
    "            # since we cannot calculate the true return\n",
    "            self.num_truncated += 1\n",
    "            return\n",
    "\n",
    "        states = episode['states']\n",
    "        actions = episode['actions']\n",
    "        rewards = episode['rewards']\n",
    "        T = len(rewards)\n",
    "\n",
    "        gamma = self.gamma\n",
    "        #######################################################################\n",
    "        # TODO: Implement every-visit Monte Carlo prediction of q as          #\n",
    "        # described in the pseudocode above.                                  #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "\n",
    "        # Reset the state values, since they need to be recomputed\n",
    "        self.v = None\n",
    "\n",
    "        # Update statistics\n",
    "        self.num_steps += len(rewards)\n",
    "        self.num_episodes += 1\n",
    "\n",
    "    def policy_evaluation(self, num_episodes, max_steps=None):\n",
    "        # Collect episodes and update the action value function\n",
    "        for _ in range(num_episodes):\n",
    "            episode = rl_util.rollout(self.env, self, max_steps=max_steps)\n",
    "            self.update_q(episode)\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        # Update the policy using epsilon-greedy policy improvement\n",
    "\n",
    "        num_s = self.num_states\n",
    "        num_a = self.num_actions\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        q = self.q\n",
    "        pi = np.zeros((num_s, num_a), dtype=float)\n",
    "        #######################################################################\n",
    "        # TODO: Implement epsilon greedy policy improvement as described in   #\n",
    "        # the pseudocode above. Remember to correctly distribute the          #\n",
    "        # probabilities across all maximizing actions.                        #\n",
    "        #######################################################################\n",
    "        \n",
    "        #######################################################################\n",
    "        # End of your code.                                                   #\n",
    "        #######################################################################\n",
    "        self.pi = pi\n",
    "\n",
    "        # Reset the state values, since they need to be recomputed\n",
    "        self.v = None\n",
    "\n",
    "    # This method is used for the GUI\n",
    "    # You don't have to understand the code\n",
    "    def interactive_optimization(self):\n",
    "        from rl_gui import RLCmd, RLParamsResult\n",
    "\n",
    "        def update_params(params):\n",
    "            self.gamma = params['gamma']\n",
    "\n",
    "            if params['epsilon'] != self.epsilon:\n",
    "                self.epsilon = params['epsilon']\n",
    "                if self.v is not None:\n",
    "                    self.policy_improvement()\n",
    "                return RLParamsResult.RENDER\n",
    "\n",
    "        yield RLCmd.Init(options={'eval': 'Evaluate episode',\n",
    "                                  'improve': 'Improve policy',\n",
    "                                  'reset': 'Reset agent'},\n",
    "                         params={'gamma': ('Discount factor', 'float', self.gamma, 0.0, 1.0 - 1e-4),\n",
    "                                 'epsilon': ('Epsilon', 'float', self.epsilon, 0.0, 1.0)},\n",
    "                         params_callback=update_params)\n",
    "\n",
    "        while True:\n",
    "            message = f'Processed {self.num_steps} steps in {self.num_episodes} episodes'\n",
    "            if self.num_truncated > 0:\n",
    "                message += f' ({self.num_truncated} truncated)'\n",
    "            option = yield RLCmd.WaitForOption(active=['eval', 'improve', 'reset'],\n",
    "                                               step='eval', interval=50, message=message)\n",
    "            if option == 'eval':\n",
    "                episode = yield RLCmd.WaitForEpisode(max_steps=20000, interval=-1)\n",
    "                self.update_q(episode)\n",
    "                #self.policy_improvement()  # TODO MC-PI vs MC-Control\n",
    "            elif option == 'improve':\n",
    "                self.policy_improvement()\n",
    "            elif option == 'reset':\n",
    "                self.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code cell to test your implementation.  \n",
    "**Important**: After changing your code, execute the above code cell before running the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mc_agent():\n",
    "    env = rl_env.default_5x5_maze(model_based=True)\n",
    "    rng = None\n",
    "\n",
    "    def seed():\n",
    "        nonlocal rng\n",
    "        rng = np.random.Generator(np.random.PCG64(seed=42))\n",
    "        env.reset(seed=42)\n",
    "\n",
    "    def create_agent(gamma, epsilon):\n",
    "        return MonteCarloAgent(env, gamma, epsilon, rng=rng)\n",
    "\n",
    "    yield 'update_q()'\n",
    "    seed()\n",
    "    for expected_sum in [4.470511, 9.752815, 6.292247]:\n",
    "        agent = create_agent(gamma=0.8, epsilon=0.01)\n",
    "        agent.policy_evaluation(num_episodes=3)\n",
    "        if (yield from rl_tests.check_numpy_array(agent.q, name='self.q', shape=(agent.num_states, agent.num_actions), dtype=np.floating)):\n",
    "            q_sum = np.sum(agent.q)\n",
    "            yield np.isclose(q_sum, expected_sum, atol=1e-5), f'The updated action values are incorrect (error = {abs(expected_sum - q_sum):.5f})'\n",
    "        yield None\n",
    "    \n",
    "    yield 'policy_improvement()'\n",
    "    seed()\n",
    "    for epsilon, expected_entropy in zip([0.01, 0.02, 0.05], [2.486858, 3.367890, 5.609687]):\n",
    "        agent = create_agent(gamma=0.8, epsilon=epsilon)\n",
    "        agent.policy_evaluation(num_episodes=5)\n",
    "        agent.policy_improvement()\n",
    "        if (yield from rl_tests.check_numpy_array(agent.pi, name='self.pi', shape=(agent.num_states, agent.num_actions), dtype=np.floating)):\n",
    "            entropy = np.sum(-agent.pi * np.log(agent.pi))\n",
    "            yield np.isclose(entropy, expected_entropy, atol=1e-5), f'The updated policy is incorrect (error = {abs(expected_entropy - entropy):.5f})'\n",
    "        yield None\n",
    "\n",
    "rl_tests.run_tests(test_mc_agent())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all of your tests passed, you can see your agent in action in the following code cell.\n",
    "\n",
    "Sometimes there is a strange bug and the environment is rendered multiple times. In that case you may have to restart the notebook and reopen the browser tab or restart the editor (e.g. in VS Code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a GUI for the maze environment\n",
    "env = rl_env.default_5x5_maze(model_based=True)\n",
    "# you can try the bigger maze by uncommenting the next line\n",
    "#env = rl_env.default_8x8_maze(model_based=True)\n",
    "\n",
    "# you can also try the TicTacToe environment,\n",
    "# in this case you need to set the render_mode below to 'ansi' (!)\n",
    "#env = rl_env.TicTacToeEnv()\n",
    "\n",
    "gamma = 0.9  # discount factor\n",
    "epsilon = 0.01\n",
    "agents = {'Random': rl_agent.RandomAgent(env),\n",
    "          'Monte Carlo control': MonteCarloAgent(env, gamma, epsilon)}\n",
    "\n",
    "rl_gui.RLGui(env, agents=agents, max_steps=1000, render_mode='rgb_array')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
